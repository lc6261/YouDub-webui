# -*- coding: utf-8 -*-
"""
WhisperX æ‰¹é‡å­—å¹•ç”Ÿæˆå·¥å…· - Silero VAD æ ¡å‡†ç‰ˆï¼ˆONNX ä¿®å¤ + å•ä½ä¿®æ­£ï¼‰
åŠŸèƒ½ï¼š
  - é«˜ç²¾åº¦è¯­éŸ³è¯†åˆ« + æ—¶é—´å¯¹é½
  - è¯´è¯äººåˆ†ç¦»ï¼ˆDiarizationï¼‰
  - æ™ºèƒ½å¥å­åˆå¹¶
  - Silero VAD æ ¡å‡†çœŸå®è¯­éŸ³æ—¶é•¿ï¼ˆONNX æ¨¡å¼ï¼Œå•ä½å·²ä¿®æ­£ï¼‰
  - ä¸­é—´æ–‡ä»¶å­˜å…¥ temp/ ç›®å½•
  
ç‰ˆæœ¬: 1.0
"""

import json
import time
import librosa
import numpy as np
import whisperx
import os
from loguru import logger
import torch
from dotenv import load_dotenv
import glob
import sys
import gc
import soundfile as sf
from whisperx.diarize import DiarizationPipeline

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
load_dotenv()

# å…¨å±€æ¨¡å‹ç¼“å­˜
whisper_model = None
diarize_model = None
align_model = None
language_code = None
align_metadata = None

# æ¨¡å‹ç¼“å­˜ç›®å½•
EXISTING_MODEL_DIR = os.getenv("HF_HUB_CACHE", r"C:\model\huggingface\hub")
DEFAULT_DOWNLOAD_ROOT = os.getenv("WHISPER_DOWNLOAD_ROOT", EXISTING_MODEL_DIR)

# ç¯å¢ƒå˜é‡è®¾ç½®
os.environ["HF_HUB_CACHE"] = EXISTING_MODEL_DIR
os.environ["TORCH_HOME"] = EXISTING_MODEL_DIR
os.environ["HUGGINGFACE_HUB_CACHE"] = EXISTING_MODEL_DIR

hf_endpoint = os.getenv("HF_ENDPOINT")
if hf_endpoint:
    os.environ["HF_ENDPOINT"] = hf_endpoint

HF_TOKEN = os.getenv("HF_TOKEN")

def init_whisperx():
    logger.info("=== WhisperX åˆå§‹åŒ–é…ç½® ===")
    logger.info(f"HF_HUB_CACHE: {os.environ.get('HF_HUB_CACHE')}")
    logger.info(f"TORCH_HOME: {os.environ.get('TORCH_HOME')}")
    
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        logger.info("âœ… å¯ç”¨TF32åŠ é€Ÿ")

# ===== ä¿®å¤ï¼šæ­£ç¡®å¤„ç† Silero VAD çš„æ¯«ç§’å•ä½ =====
def calculate_vad_duration(vad_timestamps, segment_start, segment_end):
    """è®¡ç®— VAD è¯­éŸ³æ®µåœ¨ [segment_start, segment_end] å†…çš„æ€»æ—¶é•¿ï¼ˆç§’ï¼‰"""
    total = 0.0
    for ts in vad_timestamps:
        # Silero VAD çš„ start/end å•ä½æ˜¯æ¯«ç§’ï¼ˆmsï¼‰
        vad_start = ts['start'] / 1000.0  # è½¬ä¸ºç§’
        vad_end = ts['end'] / 1000.0      # è½¬ä¸ºç§’
        
        overlap_start = max(vad_start, segment_start)
        overlap_end = min(vad_end, segment_end)
        
        if overlap_end > overlap_start:
            total += overlap_end - overlap_start
    
    return round(total, 3)
# ===============================================

def load_whisper_model(model_name='large-v3', download_root=None, device='auto'):
    global whisper_model
    if whisper_model is not None:
        return
    
    if download_root is None:
        download_root = DEFAULT_DOWNLOAD_ROOT
    
    if device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    logger.info(f'ğŸš€ åŠ è½½ WhisperX æ¨¡å‹: {model_name}')
    logger.info(f'ğŸ–¥ï¸ è®¾å¤‡: {device}')
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
    
    t_start = time.time()
    
    compute_type = "float16"
    if device == 'cpu':
        compute_type = "float32"
    elif torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory < 8 * 1024**3:
        logger.warning("âš ï¸ GPUå†…å­˜<8GBï¼Œå»ºè®®ä½¿ç”¨mediumæ¨¡å‹")
    
    whisper_model = whisperx.load_model(
        model_name, 
        download_root=download_root, 
        device=device,
        compute_type=compute_type
    )
    t_end = time.time()
    logger.info(f'âœ… WhisperX æ¨¡å‹åŠ è½½å®Œæˆ: {t_end - t_start:.2f}s')
    
    check_model_cache(download_root)

def unload_whisper_model():
    """
    å¸è½½æ‰€æœ‰ WhisperX ç›¸å…³æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
    - Whisper ä¸»æ¨¡å‹
    - å¯¹é½æ¨¡å‹
    - è¯´è¯äººåˆ†ç¦»æ¨¡å‹
    """
    global whisper_model, align_model, diarize_model, language_code, align_metadata
    
    # å¸è½½ Whisper ä¸»æ¨¡å‹
    if whisper_model is not None:
        logger.info("âœ… æ­£åœ¨å¸è½½ Whisper ä¸»æ¨¡å‹...")
        del whisper_model
        whisper_model = None
    
    # å¸è½½å¯¹é½æ¨¡å‹
    if align_model is not None:
        logger.info("âœ… æ­£åœ¨å¸è½½å¯¹é½æ¨¡å‹...")
        del align_model
        del align_metadata
        align_model = None
        align_metadata = None
        language_code = None
    
    # å¸è½½è¯´è¯äººåˆ†ç¦»æ¨¡å‹
    if diarize_model is not None:
        logger.info("âœ… æ­£åœ¨å¸è½½è¯´è¯äººåˆ†ç¦»æ¨¡å‹...")
        del diarize_model
        diarize_model = None
    
    # æ¸…ç†èµ„æº
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    
    logger.info("âœ… WhisperX ç›¸å…³æ¨¡å‹å·²å…¨éƒ¨å¸è½½")

def load_align_model(language='en', device='auto'):
    global align_model, language_code, align_metadata
    if align_model is not None and language_code == language:
        return
    
    if device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    logger.info(f'â±ï¸ åŠ è½½å¯¹é½æ¨¡å‹: {language}')
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
    
    t_start = time.time()
    align_model, align_metadata = whisperx.load_align_model(
        language_code=language, 
        device=device,
        model_dir=EXISTING_MODEL_DIR
    )
    t_end = time.time()
    logger.info(f'âœ… å¯¹é½æ¨¡å‹åŠ è½½å®Œæˆ: {t_end - t_start:.2f}s')

def load_diarize_model(device='auto'):
    global diarize_model
    if diarize_model is not None:
        return
    
    if device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    logger.info('ğŸ‘¥ åŠ è½½è¯´è¯äººåˆ†ç¦»æ¨¡å‹...')
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
    
    t_start = time.time()
    diarize_model = DiarizationPipeline(
        use_auth_token=HF_TOKEN, 
        device=device
    )
    t_end = time.time()
    logger.info(f'âœ… è¯´è¯äººåˆ†ç¦»æ¨¡å‹åŠ è½½å®Œæˆ: {t_end - t_start:.2f}s')

def check_model_cache(cache_dir):
    try:
        models_found = []
        pattern = os.path.join(cache_dir, "models--*")
        for folder in glob.glob(pattern):
            models_found.append(os.path.basename(folder))
        
        if models_found:
            logger.info(f"ğŸ“ ç¼“å­˜ä¸­æ‰¾åˆ° {len(models_found)} ä¸ªæ¨¡å‹")
            for model in models_found[:5]:
                logger.info(f"  - {model}")
    except Exception as e:
        logger.warning(f"âš ï¸ æ£€æŸ¥æ¨¡å‹ç¼“å­˜å¤±è´¥: {str(e)}")

def convert_diarization_result(diarize_segments):
    result = []
    try:
        if hasattr(diarize_segments, 'itertracks'):
            for segment, track, label in diarize_segments.itertracks(yield_label=True):
                result.append({'segment_start': segment.start, 'segment_end': segment.end, 'speaker': label})
        elif hasattr(diarize_segments, 'to_dict'):
            import pandas as pd
            df = diarize_segments
            for _, row in df.iterrows():
                result.append({'segment_start': row.get('start', 0), 'segment_end': row.get('end', 0), 'speaker': row.get('speaker', 'UNKNOWN')})
        elif isinstance(diarize_segments, (list, tuple)):
            for i, item in enumerate(diarize_segments):
                if hasattr(item, 'start') and hasattr(item, 'end'):
                    result.append({'segment_start': item.start, 'segment_end': item.end, 'speaker': getattr(item, 'speaker', f'SPEAKER_{i:02d}')})
                elif isinstance(item, dict):
                    result.append(item)
        else:
            result = {'raw_type': str(type(diarize_segments))}
    except Exception as e:
        logger.error(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        result = {'error': str(e)}
    return result

def merge_segments(transcript, ending='!"\').:;?]}~', max_gap=1.0):
    if not transcript:
        return []
    merged = []
    buffer = transcript[0].copy()
    for i in range(1, len(transcript)):
        current = transcript[i].copy()
        gap = current['start'] - buffer['end']
        should_merge = ((not buffer['text'] or buffer['text'][-1] not in ending) and gap < max_gap and current['text'].strip())
        if should_merge:
            buffer['text'] += ' ' + current['text']
            buffer['end'] = current['end']
            buffer['duration'] = round(buffer['end'] - buffer['start'], 3)
            buffer['vad_duration'] = round(buffer.get('vad_duration', 0) + current.get('vad_duration', 0), 3)
        else:
            merged.append(buffer)
            buffer = current
    if buffer:
        if 'duration' not in buffer:
            buffer['duration'] = round(buffer['end'] - buffer['start'], 3)
        if 'vad_duration' not in buffer:
            buffer['vad_duration'] = buffer['duration']
        merged.append(buffer)
    return merged

def sanitize_transcript(transcript, audio_duration):
    if not transcript:
        return []
    sanitized = []
    prev_end = 0.0
    for seg in transcript:
        try:
            start = float(seg.get('start', 0))
            end = float(seg.get('end', 0))
            text = str(seg.get('text', '')).strip()
            speaker = seg.get('speaker', 'SPEAKER_00')
            if not text or end <= start:
                continue
            start = max(0.0, start)
            end = min(audio_duration, end)
            if end <= start:
                end = start + 0.01
            if start < prev_end:
                start = prev_end
                if end <= start:
                    end = start + 0.01
            start = round(start, 3)
            end = round(end, 3)
            duration = round(end - start, 3)
            vad_duration = min(duration, seg.get('vad_duration', duration))
            sanitized.append({
                'start': start,
                'end': end,
                'duration': duration,
                'vad_duration': vad_duration,
                'text': text,
                'speaker': speaker
            })
            prev_end = end
        except (ValueError, TypeError, KeyError) as e:
            logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆæ®µ: {e}")
            continue
    return sanitized

def validate_transcript_coverage(transcript, audio_duration, wav_path, folder):
    total_duration = sum(seg['end'] - seg['start'] for seg in transcript)
    coverage_rate = total_duration / audio_duration if audio_duration > 0 else 0
    logger.info(f"ğŸ“Š æ—¶é—´è¦†ç›–åˆ†æ:")
    logger.info(f"  éŸ³é¢‘æ€»æ—¶é•¿: {audio_duration:.2f}ç§’")
    logger.info(f"  è½¬å½•æ€»æ—¶é•¿: {total_duration:.2f}ç§’")
    logger.info(f"  è¦†ç›–æ¯”ä¾‹: {coverage_rate:.1%}")
    gaps = []
    last_end = 0
    for i, seg in enumerate(transcript):
        if seg['start'] > last_end:
            gap_duration = seg['start'] - last_end
            gaps.append({'gap_index': len(gaps), 'start': last_end, 'end': seg['start'], 'duration': gap_duration})
        last_end = seg['end']
    if gaps:
        logger.warning(f"âš ï¸ å‘ç° {len(gaps)} ä¸ªæ—¶é—´ç©ºç™½")
        for gap in gaps[:3]:
            logger.warning(f"  ç©ºç™½{gap['gap_index']}: {gap['start']:.2f}-{gap['end']:.2f} ({gap['duration']:.2f}ç§’)")
    validation_report = {
        'audio_duration': audio_duration,
        'transcript_duration': total_duration,
        'coverage_rate': coverage_rate,
        'gap_count': len(gaps),
        'gaps': gaps,
        'segment_count': len(transcript)
    }
    report_path = os.path.join(folder, 'validation_report.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(validation_report, f, indent=2, ensure_ascii=False)
    logger.info(f"ğŸ“‹ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    return validation_report
def transcribe_audio(folder, model_name='large-v3', download_root=None, device='auto', 
                     batch_size=8, diarization=True, min_speakers=None, max_speakers=None):
    transcript_path = os.path.join(folder, 'transcript.json')
    if os.path.exists(transcript_path):
        logger.info(f'âœ… è½¬å½•å·²å­˜åœ¨: {transcript_path}')
        return True
    
    wav_path = os.path.join(folder, 'audio_vocals.wav')
    if not os.path.exists(wav_path):
        logger.error(f'âŒ éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {wav_path}')
        return False
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = os.path.join(folder, 'temp')
    os.makedirs(temp_dir, exist_ok=True)
    logger.info(f'ğŸ“ ä¸´æ—¶ç›®å½•: {temp_dir}')
    
    logger.info(f'ğŸ™ï¸ å¼€å§‹è½¬å½•: {wav_path}')
    
    if download_root is None:
        download_root = DEFAULT_DOWNLOAD_ROOT
    if device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # æ”¯æŒå¯¹é½çš„è¯­è¨€åˆ—è¡¨ï¼ˆWhisperX å®˜æ–¹æ”¯æŒï¼‰
    SUPPORTED_ALIGN_LANGUAGES = {
        'en', 'fr', 'de', 'es', 'it', 'pt', 'nl', 'uk', 'ja', 'zh', 'ru',
        'ar', 'cs', 'tr', 'pl', 'ca', 'hu', 'ko', 'vi', 'sw', 'sl', 'lv',
        'fi', 'ro', 'da', 'he', 'el', 'gl', 'eu', 'af', 'lt', 'pa', 'is',
        'ml', 'ms', 'mr', 'ta', 'te', 'ur', 'hi', 'bn', 'gu', 'kn', 'or'
    }

    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        
        load_whisper_model(model_name, download_root, device)
        audio_duration = librosa.get_duration(path=wav_path)
        logger.info(f'â±ï¸ éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’')
        
        if audio_duration > 600:
            batch_size = max(4, batch_size // 2)
            logger.info(f'ğŸ¬ é•¿éŸ³é¢‘ï¼Œé™ä½ batch_size åˆ° {batch_size}')
                
        # ===== Silero VAD åˆ†æï¼ˆå¼ºåˆ¶ ONNX æ¨¡å¼ï¼‰=====
        logger.info('ğŸ”Š æ‰§è¡Œ Silero VAD åˆ†æ (ONNX æ¨¡å¼)...')
        vad_timestamps = None
        try:
            from silero_vad import get_speech_timestamps, load_silero_vad
            model = load_silero_vad()
            audio_vocals, sr = librosa.load(wav_path, sr=16000)
            vad_timestamps = get_speech_timestamps(
                audio_vocals,
                model=model,
                sampling_rate=16000,
                threshold=0.5,
                min_speech_duration_ms=200,
                max_speech_duration_s=15.0,
                min_silence_duration_ms=1000,
                speech_pad_ms=200
            )
            logger.info(f'âœ… VAD æ£€æµ‹åˆ° {len(vad_timestamps)} ä¸ªè¯­éŸ³æ®µ')
        except Exception as e:
            logger.error(f"âŒ Silero VAD (ONNX) å¤±è´¥: {e}")
            logger.info("âš ï¸ å›é€€åˆ°åŸå§‹ duration")
        # ===================================
        
        logger.info('ğŸ“ è¯­éŸ³è¯†åˆ«...')
        rec_result = whisper_model.transcribe(wav_path, batch_size=batch_size)
        
        if rec_result['language'] == 'nn':
            logger.warning('â“ æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­è¨€')
            return False
        
        detected_lang = rec_result['language']
        logger.info(f'ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {detected_lang} (ç½®ä¿¡åº¦å¯èƒ½è¾ƒä½)')
        
        # ä¿å­˜åˆå§‹è½¬å½•
        initial_path = os.path.join(temp_dir, '0_initial_transcription.json')
        with open(initial_path, 'w', encoding='utf-8') as f:
            json.dump(rec_result, f, indent=2, ensure_ascii=False)
        logger.info(f'ğŸ’¾ ä¿å­˜åˆå§‹è½¬å½•: {initial_path}')
        
        # ===== å†³å®šæ˜¯å¦æ‰§è¡Œæ—¶é—´å¯¹é½ =====
        if detected_lang in SUPPORTED_ALIGN_LANGUAGES:
            logger.info('â³ æ—¶é—´å¯¹é½...')
            load_align_model(detected_lang, device)
            aligned_result = whisperx.align(
                rec_result['segments'], 
                align_model, 
                align_metadata,
                wav_path, 
                device, 
                return_char_alignments=False
            )
            aligned_path = os.path.join(temp_dir, '1_aligned_transcription.json')
            with open(aligned_path, 'w', encoding='utf-8') as f:
                json.dump(aligned_result, f, indent=2, ensure_ascii=False)
            logger.info(f'ğŸ’¾ ä¿å­˜å¯¹é½ç»“æœ: {aligned_path}')
        else:
            logger.warning(f"âš ï¸ è¯­è¨€ '{detected_lang}' ä¸åœ¨æ”¯æŒå¯¹é½çš„è¯­è¨€åˆ—è¡¨ä¸­ï¼Œè·³è¿‡å¯¹é½æ­¥éª¤")
            aligned_result = rec_result  # ç›´æ¥ä½¿ç”¨åŸå§‹ç»“æœ
        
        # ===== è¯´è¯äººåˆ†ç¦» =====
        if diarization:
            logger.info('ğŸ‘¥ è¯´è¯äººåˆ†ç¦»...')
            load_diarize_model(device)
            diarize_segments = diarize_model(
                wav_path,
                min_speakers=min_speakers, 
                max_speakers=max_speakers
            )
            
            diarize_path = os.path.join(temp_dir, '2_diarization_raw.json')
            diarize_converted = convert_diarization_result(diarize_segments)
            with open(diarize_path, 'w', encoding='utf-8') as f:
                json.dump(diarize_converted, f, indent=2, ensure_ascii=False)
            logger.info(f'ğŸ’¾ ä¿å­˜è¯´è¯äººåˆ†ç¦»ç»“æœ: {diarize_path}')
            
            assigned_result = whisperx.assign_word_speakers(diarize_segments, aligned_result)
            assigned_path = os.path.join(temp_dir, '3_assigned_speakers.json')
            with open(assigned_path, 'w', encoding='utf-8') as f:
                json.dump(assigned_result, f, indent=2, ensure_ascii=False)
            logger.info(f'ğŸ’¾ ä¿å­˜è¯´è¯äººåˆ†é…ç»“æœ: {assigned_path}')
        else:
            assigned_result = aligned_result
        
        # ===== æ„å»ºæœ€ç»ˆç»“æœï¼ˆå« VAD æ—¶é•¿ï¼‰=====
        logger.info('ğŸ”§ æ„å»ºæœ€ç»ˆç»“æœï¼ˆå« VAD æ—¶é•¿ï¼‰...')
        transcript = []
        for segment in assigned_result['segments']:
            start = float(segment.get('start', 0))
            end = float(segment.get('end', 0.01))
            if end <= start:
                end = start + 0.01
            duration = round(end - start, 3)
            
            # è®¡ç®— VAD æ—¶é•¿ï¼ˆä½¿ç”¨ä¿®å¤åçš„å•ä½è½¬æ¢ï¼‰
            if vad_timestamps is not None:
                vad_duration = calculate_vad_duration(vad_timestamps, start, end)
            else:
                vad_duration = duration
            
            transcript.append({
                'start': start,
                'end': end,
                'duration': duration,
                'vad_duration': vad_duration,
                'text': segment['text'].strip(),
                'speaker': segment.get('speaker', 'SPEAKER_00')
            })
        
        raw_transcript_path = os.path.join(temp_dir, '4_raw_transcript.json')
        with open(raw_transcript_path, 'w', encoding='utf-8') as f:
            json.dump(transcript, f, indent=2, ensure_ascii=False)
        logger.info(f'ğŸ’¾ ä¿å­˜æœªåˆå¹¶ç»“æœ: {raw_transcript_path}')
        
        # ===== åˆå¹¶ + å®‰å…¨åŒ– =====
        logger.info('ğŸ”— åˆå¹¶ç‰‡æ®µ...')
        original_count = len(transcript)
        transcript = merge_segments(transcript)
        merged_count = len(transcript)
        logger.info(f'ğŸ“Š åˆå¹¶: {original_count} â†’ {merged_count} ä¸ªç‰‡æ®µ')
        
        logger.info('ğŸ›¡ï¸ æ—¶é—´æˆ³å®‰å…¨åŒ–å¤„ç†...')
        transcript = sanitize_transcript(transcript, audio_duration)
        if not transcript:
            logger.error('âŒ å®‰å…¨åŒ–åæ— æœ‰æ•ˆå­—å¹•')
            return False
        
        # ===== éªŒè¯ + ä¿å­˜ =====
        logger.info('âœ… éªŒè¯ç»“æœ...')
        validate_transcript_coverage(transcript, audio_duration, wav_path, folder)
        
        with open(transcript_path, 'w', encoding='utf-8') as f:
            json.dump(transcript, f, indent=4, ensure_ascii=False)
        logger.info(f'âœ… è½¬å½• {len(transcript)} ä¸ªç‰‡æ®µ â†’ {transcript_path}')
        
        # ===== ç”Ÿæˆè¯´è¯äººéŸ³é¢‘ =====
        generate_speaker_audio(folder, transcript)
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        
        return True
        
    except torch.cuda.OutOfMemoryError:
        logger.error('ğŸ’¥ GPUå†…å­˜ä¸è¶³!')
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        return False
        
    except Exception as e:
        logger.exception(f'ğŸ”¥ è½¬å½•é”™è¯¯: {e}')
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        return False
def generate_speaker_audio(folder, transcript):
    """ç”Ÿæˆè¯´è¯äººéŸ³é¢‘å’Œå¯¹åº”æ–‡æœ¬ï¼ˆç”¨äº VoxCPM è¯­éŸ³å…‹éš†ï¼‰"""
    wav_path = os.path.join(folder, 'audio_vocals.wav')
    try:
        audio_data, samplerate = librosa.load(wav_path, sr=24000)
    except Exception as e:
        logger.error(f'âŒ åŠ è½½éŸ³é¢‘å¤±è´¥: {e}')
        return

    speaker_audio = {}
    speaker_texts = {}

    delay = 0.05  # å‰åæ‰©å±• 50ms

    for segment in transcript:
        start = max(0, int((segment['start'] - delay) * samplerate))
        end = min(int((segment['end'] + delay) * samplerate), len(audio_data))
        audio_chunk = audio_data[start:end]

        speaker = segment['speaker']
        text = segment['text'].strip()

        if speaker in speaker_audio:
            speaker_audio[speaker] = np.concatenate((speaker_audio[speaker], audio_chunk))
        else:
            speaker_audio[speaker] = audio_chunk

        if speaker in speaker_texts:
            speaker_texts[speaker].append(text)
        else:
            speaker_texts[speaker] = [text]

    speaker_folder = os.path.join(folder, 'SPEAKER')
    os.makedirs(speaker_folder, exist_ok=True)

    for speaker in speaker_audio:
        # === ä¿å­˜éŸ³é¢‘ï¼šç›´æ¥ä½¿ç”¨ soundfile.write ===
        wav_file = os.path.join(speaker_folder, f"{speaker}.wav")
        try:
            sf.write(wav_file, speaker_audio[speaker], 24000)
            logger.info(f'ğŸ”Š ä¿å­˜è¯´è¯äººéŸ³é¢‘: {wav_file}')
        except Exception as e:
            logger.error(f'âŒ ä¿å­˜ {speaker} éŸ³é¢‘å¤±è´¥: {e}')

        # === ä¿å­˜æ–‡æœ¬ ===
        txt_file = os.path.join(speaker_folder, f"{speaker}.txt")
        full_text = ' '.join(speaker_texts[speaker]).strip()
        try:
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(full_text)
            logger.info(f'ğŸ“ ä¿å­˜è¯´è¯äººæ–‡æœ¬: {txt_file} ({full_text[:50]}...)')
        except Exception as e:
            logger.error(f'âŒ ä¿å­˜ {speaker} æ–‡æœ¬å¤±è´¥: {e}')



def transcribe_all_audio_under_folder(folder, model_name='large-v3', download_root=None, 
                                      device='auto', batch_size=8, diarization=True, 
                                      min_speakers=None, max_speakers=None):
    logger.info(f'ğŸ“ å¼€å§‹æ‰¹é‡è½¬å½•: {folder}')
    logger.info(f'ğŸ¤– æ¨¡å‹: {model_name} | è®¾å¤‡: {device}')
    
    if download_root is None:
        download_root = DEFAULT_DOWNLOAD_ROOT
    
    folders_to_process = []
    for root, _, files in os.walk(folder):
        if 'audio_vocals.wav' in files and 'transcript.json' not in files:
            folders_to_process.append(root)
    
    logger.info(f'ğŸ¯ æ‰¾åˆ° {len(folders_to_process)} ä¸ªå¾…å¤„ç†æ–‡ä»¶å¤¹')
    
    processed, failed = 0, 0
    for i, root in enumerate(folders_to_process, 1):
        logger.info(f'\n{"â”€" * 50}')
        logger.info(f'ğŸ¬ å¤„ç† ({i}/{len(folders_to_process)}): {os.path.basename(root)}')
        try:
            if transcribe_audio(root, model_name, download_root, device, batch_size, diarization, min_speakers, max_speakers):
                processed += 1
            else:
                failed += 1
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
        except Exception as e:
            logger.exception(f'ğŸ’¥ ä¸¥é‡é”™è¯¯ {root}: {e}')
            failed += 1
    
    logger.info(f'\n{"â•" * 50}')
    logger.info(f'âœ… å®Œæˆ! æˆåŠŸ: {processed}, å¤±è´¥: {failed}')
    return f'è½¬å½• {processed} ä¸ªéŸ³é¢‘æ–‡ä»¶ (å¤±è´¥: {failed})'

def regression_test_existing_transcripts(folder):
    logger.info(f"ğŸ” å¼€å§‹å›å½’æµ‹è¯•: {folder}")
    reports = []
    for root, dirs, files in os.walk(folder):
        transcript_path = os.path.join(root, 'transcript.json')
        wav_path = os.path.join(root, 'audio_vocals.wav')
        if os.path.exists(transcript_path) and os.path.exists(wav_path):
            logger.info(f"\nğŸ“ åˆ†æ: {os.path.basename(root)}")
            try:
                with open(transcript_path, 'r', encoding='utf-8') as f:
                    transcript = json.load(f)
                audio_duration = librosa.get_duration(path=wav_path)
                report = validate_transcript_coverage(transcript, audio_duration, wav_path, root)
                report['folder'] = os.path.basename(root)
                reports.append(report)
            except Exception as e:
                logger.error(f"âŒ åˆ†æå¤±è´¥ {root}: {e}")
    if reports:
        total_folders = len(reports)
        avg_coverage = sum(r['coverage_rate'] for r in reports) / total_folders
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“ˆ å›å½’æµ‹è¯•æ±‡æ€»:")
        logger.info(f"  æ€»æ–‡ä»¶å¤¹æ•°: {total_folders}")
        logger.info(f"  å¹³å‡è¦†ç›–æ¯”ä¾‹: {avg_coverage:.1%}")
    return reports

def main():
    logger.remove()
    logger.add(
        sys.stderr,
        level="INFO",
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{message}</cyan>",
        colorize=True
    )
    logger.add(
        "whisperx_transcribe.log",
        level="DEBUG",
        rotation="10 MB",
        retention="7 days",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {message}"
    )
    
    logger.info("ğŸ”Š WhisperX æ‰¹é‡è½¬å½•å·¥å…· - Silero VAD ç‰ˆå¯åŠ¨")
    init_whisperx()
    
    target_folder = 'videos'
    if not os.path.exists(target_folder):
        logger.error(f"âŒ ç›®æ ‡æ–‡ä»¶å¤¹æœªæ‰¾åˆ°: {target_folder}")
        return
    
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if total_memory < 8:
            model_name, batch_size = 'medium', 4
            logger.warning("âš ï¸ å†…å­˜ä¸è¶³: ä½¿ç”¨ 'medium' æ¨¡å‹")
        else:
            model_name, batch_size = 'large-v3', 8
    else:
        model_name, batch_size = 'large-v3', 8
        logger.info("ğŸ’» ä½¿ç”¨ CPU")
    
    result = transcribe_all_audio_under_folder(
        target_folder, 
        model_name=model_name,
        batch_size=batch_size,
        diarization=True
    )
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ” å¯¹å·²æœ‰è½¬å½•ç»“æœè¿›è¡ŒéªŒè¯...")
    regression_test_existing_transcripts(target_folder)
    
    logger.info("\nğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼")

if __name__ == '__main__':
    main()