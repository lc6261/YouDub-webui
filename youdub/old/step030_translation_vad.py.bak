# -*- coding: utf-8 -*-
"""
è§†é¢‘å­—å¹•ç¿»è¯‘å·¥å…· - å…¨å±€åˆ†æå¢å¼ºç‰ˆï¼ˆæœ¯è¯­ + é£æ ¼ + ä¸Šä¸‹æ–‡ï¼‰
æ”¯æŒï¼šOpenAI API, LM Studio, Ollama
ç‰¹æ€§ï¼š
  âœ… å…¨å±€åˆ†æï¼šæœ¯è¯­è¡¨ + é£æ ¼ + æ‘˜è¦
  âœ… å¼ºåˆ¶ JSON è¾“å‡ºï¼ˆ{"translation": "..."}ï¼‰
  âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆå±€éƒ¨ + å…¨å±€ï¼‰
  âœ… VAD æ—¶é•¿æ„ŸçŸ¥é•¿åº¦æ§åˆ¶
  âœ… ä¸¥æ ¼æ—¶é—´æˆ³å¯¹é½
  âœ… è‡ªåŠ¨é‡è¯• + å›é€€æœºåˆ¶
  âœ… è¯¦ç»†çš„ç¿»è¯‘ç»Ÿè®¡
"""

import json
import os
import re
import sys
import time
import traceback
import socket
from enum import Enum
from typing import Optional, List, Dict, Any, Tuple
from openai import OpenAI
from dotenv import load_dotenv
from loguru import logger

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# === é…ç½®æ¨¡å‹ç±»å‹ ===
class ModelType(Enum):
    OPENAI = "openai"
    LM_STUDIO = "lm_studio"
    OLLAMA = "ollama"

MODEL_NAME = os.getenv('MODEL_NAME', 'llama3.1:8b').strip()
API_BASE = os.getenv('OPENAI_API_BASE', '').strip()
API_KEY = os.getenv('OPENAI_API_KEY', '').strip()

def detect_model_type() -> ModelType:
    api_base_lower = API_BASE.lower()
    if not API_BASE:
        return ModelType.OLLAMA
    if 'localhost' in api_base_lower or '127.0.0.1' in api_base_lower:
        if 'lm-studio' in api_base_lower or ':1234' in API_BASE:
            return ModelType.LM_STUDIO
        elif 'ollama' in api_base_lower or ':11434' in API_BASE:
            return ModelType.OLLAMA
        else:
            return ModelType.OLLAMA
    return ModelType.OPENAI

MODEL_TYPE = detect_model_type()

# === é…ç½®å®¢æˆ·ç«¯ ===
if MODEL_TYPE == ModelType.OLLAMA:
    if not API_BASE:
        API_BASE = 'http://127.0.0.1:11434/v1'
    if not API_KEY:
        API_KEY = 'ollama'
    if 'localhost' in API_BASE.lower():
        API_BASE = API_BASE.replace('localhost', '127.0.0.1')
    logger.info(f"âœ… ä½¿ç”¨Ollamaæ¨¡å‹: {MODEL_NAME}")
elif MODEL_TYPE == ModelType.LM_STUDIO:
    logger.info(f"ğŸ“¦ ä½¿ç”¨LM Studioæ¨¡å‹: {MODEL_NAME}")
else:
    logger.info(f"â˜ï¸  ä½¿ç”¨OpenAIå…¼å®¹APIæ¨¡å‹: {MODEL_NAME}")
logger.info(f"ğŸŒ APIåœ°å€: {API_BASE}")

SPLIT_SENTENCES = False
CONTEXT_WINDOW = int(os.getenv('CONTEXT_WINDOW', 2))
logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡çª—å£: å‰åå„ {CONTEXT_WINDOW} å¥")

_client: Optional[OpenAI] = None

def create_openai_client() -> OpenAI:
    timeout = 180.0 if MODEL_TYPE == ModelType.OLLAMA else 120.0
    return OpenAI(base_url=API_BASE, api_key=API_KEY, timeout=timeout, max_retries=2)

def get_client() -> OpenAI:
    global _client
    if _client is None:
        _client = create_openai_client()
    return _client

# === å¥åº·æ£€æŸ¥ï¼ˆç•¥ï¼Œä¿æŒä¸å˜ï¼‰===
def check_model_health() -> bool:
    try:
        port = int(re.search(r':(\d+)', API_BASE).group(1)) if ':' in API_BASE else 11434
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
            sock.settimeout(3)
            if sock.connect_ex(('127.0.0.1', port)) != 0:
                logger.error(f"âŒ ç«¯å£ {port} æœªå¼€æ”¾")
                return False
        logger.info(f"âœ… ç«¯å£ {port} å¼€æ”¾")
    except Exception as e:
        logger.error(f"âŒ ç«¯å£æ£€æŸ¥å¤±è´¥: {e}")
        return False

    import requests
    test_url = API_BASE.replace('/v1', '/api/tags') if MODEL_TYPE == ModelType.OLLAMA else f"{API_BASE}/models"
    try:
        resp = requests.get(test_url, timeout=5)
        if resp.status_code == 200:
            logger.info("âœ… æ¨¡å‹æœåŠ¡å¥åº·")
            return True
    except:
        pass
    logger.error("âŒ æ¨¡å‹æœåŠ¡ä¸å¯ç”¨")
    return False

def get_necessary_info(info: dict) -> dict:
    return {k: info.get(k, '') for k in ['title', 'uploader', 'description', 'upload_date', 'categories', 'tags']}

# âœ… æ–°å¢ï¼šå…¨å±€åˆ†æå‡½æ•°
def analyze_transcript(transcript: list, target_language: str = 'ç®€ä½“ä¸­æ–‡', max_retries: int = 3) -> dict:
    full_text = ' '.join(line['text'] for line in transcript)
    # æˆªæ–­åˆ° 3000 å­—ï¼ˆå¹³è¡¡ä¸Šä¸‹æ–‡ä¸ Ollama èƒ½åŠ›ï¼‰
    input_text = full_text[:3000]
    
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šæŠ€æœ¯å†…å®¹åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹è§†é¢‘å­—å¹•å…¨æ–‡ï¼Œè¾“å‡ºæ ‡å‡† JSONã€‚

è¦æ±‚ï¼š
1. ç”Ÿæˆç®€æ´æ‘˜è¦ï¼ˆ100å­—å†…ï¼‰
2. æå–å…³é”®æœ¯è¯­ï¼ˆè‹±æ–‡ â†’ {target_language}ï¼‰ï¼Œè‡³å°‘5ä¸ªï¼Œä¼˜å…ˆæ•°å­¦/æŠ€æœ¯è¯
3. åˆ¤æ–­è¯­è¨€é£æ ¼ï¼ˆå¦‚â€œå­¦æœ¯è®²è§£â€ã€â€œè½»æ¾ç§‘æ™®â€ã€â€œä¸¥è°¨æ¨å¯¼â€ï¼‰
4. åˆ—å‡º2-3æ¡ç¿»è¯‘æ³¨æ„äº‹é¡¹ï¼ˆå¦‚ä»£è¯æŒ‡ä»£ã€æ–‡åŒ–ç‰¹å®šè¡¨è¾¾ï¼‰

å­—å¹•å…¨æ–‡ï¼ˆèŠ‚é€‰ï¼‰:
{input_text}

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
  "summary": "æ‘˜è¦æ–‡æœ¬",
  "style": "é£æ ¼æè¿°",
  "terms": {{"Laplace transform": "æ‹‰æ™®æ‹‰æ–¯å˜æ¢", "ODE": "å¸¸å¾®åˆ†æ–¹ç¨‹"}},
  "translation_notes": ["æ³¨æ„ 'it' æŒ‡ä»£ç³»ç»Ÿå“åº”", "é¿å…ç›´è¯‘ 'elegant'"]
}}"""

    system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚"
    client = get_client()
    model_params = {
        'temperature': 0.3,
        'top_p': 0.9,
        'frequency_penalty': 0.0,
        'presence_penalty': 0.0,
    }

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role":"system","content":system_prompt},{"role":"user","content":prompt}],
                max_tokens=500,
                timeout=90,
                **model_params
            )
            raw = response.choices[0].message.content.strip()
            logger.debug(f"ğŸ§  å…¨å±€åˆ†æåŸå§‹è¾“å‡º: {raw[:200]}...")

            # å®‰å…¨è§£æ
            try:
                data = json.loads(raw)
            except:
                # æ­£åˆ™ fallback
                summary_match = re.search(r'"summary"\s*:\s*"((?:[^"\\]|\\.)*))', raw)
                style_match = re.search(r'"style"\s*:\s*"((?:[^"\\]|\\.)*))', raw)
                terms_match = re.search(r'"terms"\s*:\s*(\{[^}]*\})', raw)
                notes_match = re.search(r'"translation_notes"\s*:\s*(\[[^\]]*\])', raw)
                
                data = {
                    "summary": summary_match.group(1).replace('\\"', '"') if summary_match else "æ‘˜è¦ç”Ÿæˆå¤±è´¥",
                    "style": style_match.group(1).replace('\\"', '"') if style_match else "æœªçŸ¥é£æ ¼",
                    "terms": json.loads(terms_match.group(1).replace('\\"', '"')) if terms_match else {},
                    "translation_notes": json.loads(notes_match.group(1).replace('\\"', '"')) if notes_match else []
                }

            # ç¡®ä¿å­—æ®µå­˜åœ¨
            return {
                "summary": data.get("summary", "æ‘˜è¦ç”Ÿæˆå¤±è´¥"),
                "style": data.get("style", "ä¸“ä¸š"),
                "terms": data.get("terms", {}),
                "translation_notes": data.get("translation_notes", [])
            }
        except Exception as e:
            logger.warning(f"âš ï¸ å…¨å±€åˆ†æå¤±è´¥ (ç¬¬{attempt+1}æ¬¡): {e}")
            if attempt < max_retries - 1:
                time.sleep(3)
    
    return {
        "summary": "å…¨å±€åˆ†æå¤±è´¥",
        "style": "ä¸“ä¸š",
        "terms": {},
        "translation_notes": []
    }

# âœ… å¢å¼ºç‰ˆ Prompt æ„å»º
def build_enhanced_prompt(
    current_text: str,
    prev_texts: List[str],
    next_texts: List[str],
    actual_duration: float,
    global_analysis: dict,
    target_language: str = 'ç®€ä½“ä¸­æ–‡'
) -> str:
    max_chars = estimate_max_chars(actual_duration)
    
    # æ„å»ºæœ¯è¯­å­—ç¬¦ä¸²
    terms = global_analysis.get('terms', {})
    term_list = [f"{k} â†’ {v}" for k, v in list(terms.items())[:10]]  # æœ€å¤š10ä¸ªæœ¯è¯­
    term_str = "ï¼›".join(term_list) if term_list else "æ— ç‰¹å®šæœ¯è¯­"

    # æ„å»ºæ³¨æ„äº‹é¡¹
    notes = global_analysis.get('translation_notes', [])
    notes_str = "ï¼›".join(notes[:3]) if notes else "æ— ç‰¹æ®Šæ³¨æ„äº‹é¡¹"

    style = global_analysis.get('style', 'ä¸“ä¸š')
    
    # æ„å»ºä¸Šä¸‹æ–‡
    context_parts = []
    for i, text in enumerate(prev_texts, 1):
        if text: context_parts.append(f"å‰{i}å¥: {text}")
    context_parts.append(f"å½“å‰å¥: {current_text}")
    for i, text in enumerate(next_texts, 1):
        if text: context_parts.append(f"å{i}å¥: {text}")
    context_str = "\n".join(context_parts)

    return f"""ä½ æ­£åœ¨ç¿»è¯‘ä¸€ä¸ª{style}é£æ ¼çš„è§†é¢‘å­—å¹•ã€‚è¯·å°†è‹±æ–‡ç¿»è¯‘æˆåœ°é“ã€æµç•…çš„{target_language}ã€‚

# å…¨å±€æŒ‡å¯¼
- æœ¯è¯­è¡¨ï¼ˆå¿…é¡»éµå®ˆï¼‰: {term_str}
- ç¿»è¯‘æ³¨æ„äº‹é¡¹: {notes_str}

# ç¿»è¯‘è¦æ±‚
1. ä¸¥æ ¼è¾“å‡º JSON: {{"translation": "è¯‘æ–‡"}}
2. è¯‘æ–‡é•¿åº¦åŒ¹é…è¯­éŸ³ï¼ˆçº¦{actual_duration:.1f}ç§’ï¼‰ï¼Œæœ€å¤š{max_chars}ä¸ªæ±‰å­—
3. ä½¿ç”¨ä¸Šè¿°æœ¯è¯­è¡¨ï¼Œä¿æŒå…¨æ–‡ä¸€è‡´
4. åªç¿»è¯‘â€œå½“å‰å¥â€ï¼Œä¸è¦ç¿»è¯‘ä¸Šä¸‹æ–‡

# ä¸Šä¸‹æ–‡
{context_str}

è¯·è¾“å‡º JSONï¼š"""

# ä¿ç•™ estimate_max_charsï¼ˆç•¥ï¼‰
def estimate_max_chars(actual_duration: float) -> int:
    return max(8, min(120, int(actual_duration * 4.5)))

def get_model_params():
    return {
        'temperature': 0.1,
        'top_p': 0.9,
        'frequency_penalty': 0.0,
        'presence_penalty': 0.0,
    }

# âœ… ä¿®æ”¹ï¼šæ”¯æŒå…¨å±€åˆ†æ
def _translate_single_with_json_retry(
    current_text: str,
    prev_texts: List[str],
    next_texts: List[str],
    actual_duration: float,
    target_language: str,
    global_analysis: dict,  # â† æ–°å¢å‚æ•°
    max_retries: int = 3
) -> Tuple[str, bool]:
    client = get_client()
    model_params = get_model_params()
    system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šç¿»è¯‘ã€‚ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦ç¿»è¯‘ä¸Šä¸‹æ–‡éƒ¨åˆ†ã€‚"
    
    for attempt in range(max_retries):
        try:
            user_prompt = build_enhanced_prompt(
                current_text, prev_texts, next_texts, actual_duration, global_analysis, target_language
            )
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=250,
                timeout=60,
                **model_params
            )
            raw_output = response.choices[0].message.content.strip()
            logger.debug(f"ğŸ”„ åŸå§‹è¾“å‡º: {raw_output[:200]}...")
            
            # å¤ç”¨åŸæœ‰è§£æå™¨
            translation = safe_json_parse_translation(raw_output)
            if translation:
                logger.debug(f"âœ… è§£ææˆåŠŸ: {translation[:100]}...")
                return translation, True
            else:
                logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥ (ç¬¬{attempt+1}æ¬¡): {raw_output[:100]}...")
        except Exception as e:
            logger.warning(f"âš ï¸ è¯·æ±‚å¤±è´¥ (ç¬¬{attempt+1}æ¬¡): {e}")
        if attempt < max_retries - 1:
            time.sleep(1.5)
    
    return current_text, False

# âœ… ä¸»ç¿»è¯‘å‡½æ•°ï¼šä¼ å…¥å…¨å±€åˆ†æ
def _translate_with_analysis(
    summary: dict,
    transcript: list,
    global_analysis: dict,
    target_language: str = 'ç®€ä½“ä¸­æ–‡'
) -> tuple:
    total_lines = len(transcript)
    logger.info(f"ğŸ“Š å¼€å§‹ç¿»è¯‘ï¼ˆå«å…¨å±€åˆ†æï¼‰ï¼Œå…± {total_lines} å¥")
    
    full_translation = []
    success_flags = []

    for idx, line in enumerate(transcript):
        text = line.get('text', '').strip()
        if not text:
            full_translation.append("")
            success_flags.append(False)
            continue

        prev_texts = [transcript[i].get('text', '') for i in range(max(0, idx-CONTEXT_WINDOW), idx)]
        next_texts = [transcript[i].get('text', '') for i in range(idx+1, min(len(transcript), idx+CONTEXT_WINDOW+1))]

        original_duration = float(line.get('end', 0)) - float(line.get('start', 0))
        vad_duration = line.get('vad_duration')
        actual_duration = min(float(vad_duration), original_duration) if vad_duration is not None else original_duration

        progress = (idx + 1) / total_lines * 100
        logger.info(f"ğŸ“ˆ è¿›åº¦: {idx+1}/{total_lines} ({progress:.1f}%) ({actual_duration:.1f}s) - {text[:60]}...")

        translation, success = _translate_single_with_json_retry(
            current_text=text,
            prev_texts=prev_texts,
            next_texts=next_texts,
            actual_duration=actual_duration,
            target_language=target_language,
            global_analysis=global_analysis,  # ä¼ å…¥å…¨å±€åˆ†æ
            max_retries=3
        )
        
        full_translation.append(translation)
        success_flags.append(success)
        
        logger.info(f"ğŸ“– [{idx+1:3d}] åŸæ–‡: {text}")
        logger.info(f"ğŸ’¬ è¯‘æ–‡: {translation}")
        logger.info(f"     æ—¶é•¿: {actual_duration:.1f}s | çŠ¶æ€: {'âœ…' if success else 'âŒ'}")
        logger.info("-" * 60)
        
        if MODEL_TYPE == ModelType.OLLAMA:
            time.sleep(0.3)
        else:
            time.sleep(0.1)

    return full_translation, success_flags

# === ä»¥ä¸‹å‡½æ•°ä¿æŒä¸å˜ ===
def safe_json_parse_translation(intext: str) -> str:
    if not intext:
        return ""
    text = intext.replace('â€}', '"}').replace('â€œ}', '"}').replace('â€]', '"]').replace('â€œ]', '"]')
    try:
        data = json.loads(text)
        if isinstance(data, dict) and 'translation' in data:
            return str(data['translation']).strip()
    except:
        pass
    json_match = re.search(r'\{[^{}]*"translation"[^{}]*"[^"]*"[^{}]*\}', text)
    if json_match:
        json_str = json_match.group(0)
        try:
            data = json.loads(json_str)
            if 'translation' in data:
                return str(data['translation']).strip()
        except:
            pass
    trans_match = re.search(r'"translation"\s*:\s*"((?:[^"\\]|\\.)*))', text)
    if trans_match:
        extracted = trans_match.group(1)
        extracted = extracted.replace('\\"', '"').replace('\\', '')
        return extracted.strip()
    start_pos = text.find('"translation"')
    if start_pos != -1:
        brace_start = text.rfind('{', 0, start_pos)
        if brace_start != -1:
            brace_count = 0
            for i, char in enumerate(text[brace_start:], brace_start):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        potential_json = text[brace_start:i+1]
                        try:
                            data = json.loads(potential_json)
                            if 'translation' in data:
                                return str(data['translation']).strip()
                        except:
                            pass
    return ""

def translation_postprocess(result: str) -> str:
    if not result:
        return ""
    result = re.sub(r'\ï¼ˆ[^)]*\ï¼‰', '', result)
    result = re.sub(r'\([^)]*\)', '', result)
    result = result.replace('...', 'ï¼Œ').replace('..', 'ï¼Œ')
    result = re.sub(r'(?<=\d),(?=\d)', '', result)
    result = result.replace('ï¼Ÿ', '?').replace('ï¼', '!')
    replacements = {
        'Â²': 'çš„å¹³æ–¹', 'â€”â€”â€”â€”': 'ï¼š', 'â€”â€”': 'ï¼š', 'Â°': 'åº¦',
        'AI': 'äººå·¥æ™ºèƒ½', 'transformer': 'Transformer', 'GPT': 'GPT',
        'LLM': 'å¤§è¯­è¨€æ¨¡å‹', '\u200b': '', '\ufeff': '',
    }
    for old, new in replacements.items():
        result = result.replace(old, new)
    return re.sub(r'\s+', ' ', result).strip()

# ... [å…¶ä½™å‡½æ•°ä¿æŒä¸å˜ï¼šsplit_text_into_sentences, split_sentences_fixed, fix_timestamps, validate_time_alignment, print_translation_preview] ...

def split_text_into_sentences(para: str) -> List[str]:
    if not para: return [para] if para else []
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ?!\.])\s+', para)
    return [s.strip() for s in sentences if s.strip()] or [para.strip()]

def split_sentences_fixed(translation: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    output_data = []
    for item in translation:
        try:
            original_start = float(item.get('start', 0))
            original_end = float(item.get('end', 0))
            text = item.get('text', '')
            speaker = item.get('speaker', '')
            translation_text = item.get('translation', '').strip()
            if not translation_text:
                continue
            sentences = split_text_into_sentences(translation_text)
            if len(sentences) <= 1:
                output_data.append({
                    "start": round(original_start, 3),
                    "end": round(original_end, 3),
                    "text": text,
                    "speaker": speaker,
                    "translation": translation_text if sentences else ""
                })
                continue
            total_sentences = len(sentences)
            original_duration = original_end - original_start
            duration_per_sentence = (original_duration / total_sentences) if original_duration > 0 else 1.0 / total_sentences
            current_time = original_start
            for i, sentence in enumerate(sentences):
                if not sentence: continue
                end_time = original_end if i == total_sentences - 1 else current_time + duration_per_sentence
                end_time = min(end_time, original_end)
                if end_time <= current_time:
                    end_time = current_time + 0.01
                output_data.append({
                    "start": round(current_time, 3),
                    "end": round(end_time, 3),
                    "text": text,
                    "speaker": speaker,
                    "translation": sentence
                })
                current_time = end_time
        except Exception as e:
            logger.error(f"âŒ å¥å­æ‹†åˆ†å‡ºé”™: {e}")
            output_data.append({
                "start": round(original_start, 3),
                "end": round(original_end, 3),
                "text": text,
                "speaker": speaker,
                "translation": translation_text
            })
    return output_data

def fix_timestamps(original: List[Dict[str, Any]], translated: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    if not original: return translated
    key_map = {(item['text'], item.get('speaker', '')): (item['start'], item['end']) for item in original}
    fixed = []
    for item in translated:
        key = (item['text'], item.get('speaker', ''))
        start, end = key_map.get(key, (item.get('start', 0), item.get('end', 0)))
        fixed.append({
            "start": round(float(start), 3),
            "end": round(float(end), 3),
            "text": item["text"],
            "speaker": item.get("speaker", ""),
            "translation": item.get("translation", item["text"])
        })
    return fixed

def validate_time_alignment(original: List[Dict[str, Any]], translated: List[Dict[str, Any]]) -> bool:
    if len(original) != len(translated):
        logger.warning(f"âŒ æ¡ç›®æ•°é‡ä¸åŒ¹é…: {len(original)} vs {len(translated)}")
        return False
    for i, (orig, trans) in enumerate(zip(original, translated)):
        os, oe = round(float(orig['start']), 3), round(float(orig['end']), 3)
        ts, te = round(float(trans['start']), 3), round(float(trans['end']), 3)
        if abs(os - ts) > 0.001 or abs(oe - te) > 0.001:
            logger.warning(f"âš ï¸ æ—¶é—´æˆ³ä¸åŒ¹é… {i}: {os}-{oe} vs {ts}-{te}")
            return False
    logger.info("âœ… æ—¶é—´æˆ³å®Œå…¨ä¸€è‡´")
    return True

def print_translation_preview(translated_data: List[Dict[str, Any]], num_lines: int = 10):
    logger.info(f"\nğŸ” ç¿»è¯‘ç»“æœé¢„è§ˆï¼ˆå‰ {num_lines} æ¡ï¼‰:")
    logger.info("="*80)
    for i, item in enumerate(translated_data[:num_lines]):
        start_time = item.get('start', 0)
        end_time = item.get('end', 0)
        original_text = item.get('text', '')
        translated_text = item.get('translation', '')
        
        logger.info(f"[{i+1:2d}] {start_time:.1f}s - {end_time:.1f}s")
        logger.info(f"     åŸæ–‡: {original_text}")
        logger.info(f"     è¯‘æ–‡: {translated_text}")
        logger.info("-" * 80)
    logger.info(f"âœ… å…± {len(translated_data)} æ¡ç¿»è¯‘å®Œæˆ")

# === ä¸»æµç¨‹ï¼šé›†æˆå…¨å±€åˆ†æ ===
def translate(folder: str, target_language: str = 'ç®€ä½“ä¸­æ–‡') -> bool:
    try:
        translation_path = os.path.join(folder, 'translation.json')
        if os.path.exists(translation_path):
            logger.info(f"ğŸ“‚ å·²å­˜åœ¨ç¿»è¯‘ï¼Œè·³è¿‡: {folder}")
            return True

        info_path = os.path.join(folder, 'download.info.json')
        transcript_path = os.path.join(folder, 'transcript.json')
        if not os.path.exists(info_path) or not os.path.exists(transcript_path):
            logger.error(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶")
            return False

        with open(info_path, 'r', encoding='utf-8') as f:
            info = get_necessary_info(json.load(f))
        with open(transcript_path, 'r', encoding='utf-8') as f:
            original_transcript = json.load(f)
        logger.info(f"ğŸ“„ åŠ è½½äº† {len(original_transcript)} æ¡åŸå§‹å­—å¹•")

        # ğŸ”¥ æ–°å¢ï¼šå…¨å±€åˆ†æ
        analysis_path = os.path.join(folder, 'analysis.json')
        if os.path.exists(analysis_path):
            try:
                with open(analysis_path, 'r', encoding='utf-8') as f:
                    global_analysis = json.load(f)
                logger.info("ğŸ§  åŠ è½½å…¨å±€åˆ†æç»“æœ")
            except:
                global_analysis = analyze_transcript(original_transcript, target_language)
                with open(analysis_path, 'w', encoding='utf-8') as f:
                    json.dump(global_analysis, f, indent=2, ensure_ascii=False)
        else:
            global_analysis = analyze_transcript(original_transcript, target_language=target_language)
            with open(analysis_path, 'w', encoding='utf-8') as f:
                json.dump(global_analysis, f, indent=2, ensure_ascii=False)
            logger.info("ğŸ” å…¨å±€åˆ†æå®Œæˆ")

        # æ‘˜è¦å¯å¤ç”¨ analysis ä¸­çš„ summary
        summary = {
            'title': info.get('title', ''),
            'author': info.get('uploader', ''),
            'summary': global_analysis.get('summary', 'æ‘˜è¦ç”Ÿæˆå¤±è´¥'),
            'tags': info.get('tags', []),
            'language': target_language,
            'model_type': MODEL_TYPE.value
        }
        # ä¿å­˜æ‘˜è¦ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
        summary_path = os.path.join(folder, 'summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        # ğŸ”¥ ä½¿ç”¨å¢å¼ºç¿»è¯‘
        logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½ç¿»è¯‘ï¼ˆå«å…¨å±€åˆ†æï¼‰...")
        translations, success_flags = _translate_with_analysis(
            summary, original_transcript, global_analysis, target_language
        )

        # æ„å»ºç»“æœï¼ˆåç»­ä¿æŒä¸å˜ï¼‰
        working_transcript = []
        for i, line in enumerate(original_transcript):
            trans_text = translations[i] if i < len(translations) else line.get('text', '')
            working_transcript.append({
                "start": float(line.get('start', 0)),
                "end": float(line.get('end', 0)),
                "text": line.get('text', ''),
                "speaker": line.get('speaker', ''),
                "translation": translation_postprocess(trans_text)
            })

        if SPLIT_SENTENCES:
            final_transcript = split_sentences_fixed(working_transcript)
        else:
            final_transcript = working_transcript

        final_transcript = fix_timestamps(original_transcript, final_transcript)
        validate_time_alignment(original_transcript, final_transcript)

        with open(translation_path, 'w', encoding='utf-8') as f:
            json.dump(final_transcript, f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… ç¿»è¯‘å®Œæˆ: {translation_path}")

        print_translation_preview(final_transcript, num_lines=10)

        total = len(success_flags)
        success_count = sum(success_flags)
        stats = {
            'total_lines': total,
            'success_count': success_count,
            'failure_count': total - success_count,
            'success_rate': round(100 * success_count / total, 2) if total else 0,
            'failed_lines': [
                {'index': i, 'start': line['start'], 'end': line['end'], 'text': line['text'][:200]}
                for i, (line, ok) in enumerate(zip(original_transcript, success_flags)) if not ok
            ]
        }
        with open(os.path.join(folder, 'translation_stats.json'), 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ“Š æˆåŠŸç‡: {stats['success_rate']:.1f}% ({success_count}/{total})")

        return True
    except Exception as e:
        logger.error(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return False

# ... [å…¶ä½™å‡½æ•°ä¿æŒä¸å˜ï¼štranslate_all_transcript_under_folder, __main__] ...

def translate_all_transcript_under_folder(root_folder: str, target_language: str = 'ç®€ä½“ä¸­æ–‡') -> int:
    if not check_model_health():
        return 0
    video_folders = [
        root for root, _, files in os.walk(root_folder)
        if 'transcript.json' in files and 'translation.json' not in files
    ]
    logger.info(f"ğŸ¯ æ‰¾åˆ° {len(video_folders)} ä¸ªéœ€è¦ç¿»è¯‘çš„è§†é¢‘")
    if not video_folders:
        logger.info("âœ… æ‰€æœ‰è§†é¢‘éƒ½å·²ç¿»è¯‘å®Œæˆ")
        return 0

    count = 0
    for i, folder in enumerate(video_folders):
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¬ å¤„ç†è§†é¢‘ ({i+1}/{len(video_folders)}): {folder}")
        if translate(folder, target_language):
            count += 1
            logger.info("âœ… å®Œæˆ")
        else:
            logger.error("âŒ å¤±è´¥")
        if i < len(video_folders) - 1:
            time.sleep(1)
    logger.info(f"\nğŸ å…±æˆåŠŸç¿»è¯‘ {count} ä¸ªè§†é¢‘")
    return count

if __name__ == '__main__':
    logger.remove()
    logger.add(sys.stderr, level="INFO", format="<green>{time:MM-DD HH:mm:ss}</green> | <level>{level: <6}</level> | <cyan>{message}</cyan>")
    logger.add("translation.log", level="DEBUG", rotation="10 MB", retention="7 days", encoding="utf-8")

    print("\n" + "="*60)
    print("ğŸ¬ è§†é¢‘å­—å¹•ç¿»è¯‘å·¥å…· - å…¨å±€åˆ†æå¢å¼ºç‰ˆ")
    print("="*60)
    logger.info(f"ğŸ“¦ æ¨¡å‹: {MODEL_TYPE.value} | {MODEL_NAME}")
    logger.info(f"ğŸŒ API: {API_BASE}")
    logger.info(f"ğŸ”§ å¥å­æ‹†åˆ†: {'å¯ç”¨' if SPLIT_SENTENCES else 'ç¦ç”¨'}")
    logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡çª—å£: å‰åå„ {CONTEXT_WINDOW} å¥")

    success_count = translate_all_transcript_under_folder('videos', 'ç®€ä½“ä¸­æ–‡')
    if success_count:
        logger.info(f"ğŸ‰ æˆåŠŸç¿»è¯‘ {success_count} ä¸ªè§†é¢‘")
    else:
        logger.info("â„¹ï¸  æ— æ–°è§†é¢‘æˆ–å…¨éƒ¨å¤±è´¥")