#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§è§†é¢‘å­—å¹•ç¿»è¯‘æ¨¡å—ï¼ˆQwen2.5 + å¤šè½®æ ¡å¯¹ + æ™ºèƒ½æœ¯è¯­åº“ + è‡ªåŠ¨å…‹éš†éŸ³é¢‘æå–ï¼‰

æ–°å¢åŠŸèƒ½ï¼š
âœ… è‡ªåŠ¨ä» audio_vocals.wav æå–æ¯ä¸ªè¯´è¯äºº â‰¤10 ç§’çš„å¹²å‡€è¯­éŸ³ç‰‡æ®µ
âœ… ä¿å­˜ä¸º SPEAKER/SPEAKER_XX_CLONE.wav ä¾› VoxCPM ä½¿ç”¨

ä½œè€…: Advanced Translation Team
æ—¥æœŸ: 2026-01-04
ç‰ˆæœ¬: 2.1
"""

import json
import os
import re
import sys
import time
from typing import List, Dict, Tuple, Optional
from openai import OpenAI
from loguru import logger
from dotenv import load_dotenv

# å°è¯•å¯¼å…¥éŸ³é¢‘å¤„ç†åº“ï¼ˆç”¨äºè‡ªåŠ¨æå–å…‹éš†éŸ³é¢‘ï¼‰
try:
    import librosa
    import numpy as np
    HAS_LIBROSA = True
except ImportError:
    HAS_LIBROSA = False
    logger.warning("âš ï¸ librosa æœªå®‰è£…ï¼Œå°†è·³è¿‡è‡ªåŠ¨å…‹éš†éŸ³é¢‘æå–")

load_dotenv()

# ===== é…ç½® =====
MODEL_NAME = os.getenv('MODEL_NAME', 'qwen2.5:14b')
API_BASE = os.getenv('OPENAI_API_BASE', 'http://127.0.0.1:11434/v1')
API_KEY = os.getenv('OPENAI_API_KEY', 'ollama')

logger.info(f"ğŸ¤– ä½¿ç”¨ç¿»è¯‘æ¨¡å‹: {MODEL_NAME}")
logger.info(f"ğŸŒ APIåœ°å€: {API_BASE}")

_client: Optional[OpenAI] = None

def get_client() -> OpenAI:
    global _client
    if _client is None:
        _client = OpenAI(base_url=API_BASE, api_key=API_KEY, timeout=120.0)
    return _client


# ===== å…¨å±€åˆ†æå™¨ï¼ˆæ–°å¢ï¼‰=====
def analyze_transcript(transcript: List[Dict], target_language: str = 'ç®€ä½“ä¸­æ–‡') -> Dict[str, str]:
    full_text = ' '.join(line.get('text', '') for line in transcript[:50])
    input_text = full_text[:3000]

    prompt = f"""ä½ æ˜¯ä¸“ä¸šè§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚è¯·ä»ä»¥ä¸‹å­—å¹•ä¸­æå–å…³é”®æœ¯è¯­ï¼ˆè‹±æ–‡ â†’ {target_language}ï¼‰ã€‚

è¦æ±‚ï¼š
1. æå–ä¸“æœ‰åè¯ã€åœ°åã€äººåã€æ–‡åŒ–æ¦‚å¿µã€æŠ€æœ¯æœ¯è¯­ç­‰
2. è¾“å‡ºæ ‡å‡†JSONæ ¼å¼

å­—å¹•å†…å®¹:
{input_text}

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
  "Paris": "å·´é»",
  "UNESCO": "è”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡",
  "Northern Lights": "åŒ—æå…‰"
}}"""

    try:
        client = get_client()
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿æå–å¤šé¢†åŸŸæœ¯è¯­ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=600
        )
        
        raw = response.choices[0].message.content.strip()
        logger.debug(f"å…¨å±€åˆ†ææœ¯è¯­åŸå§‹è¾“å‡º: {raw[:200]}...")

        try:
            terms = json.loads(raw)
        except:
            terms = {}
            matches = re.findall(r'"([^"]+)"\s*:\s*"([^"]+)"', raw)
            for en, zh in matches:
                if en and zh:
                    terms[en] = zh

        logger.info(f"ğŸ§  å…¨å±€åˆ†ææå–åˆ° {len(terms)} ä¸ªæœ¯è¯­")
        return terms

    except Exception as e:
        logger.warning(f"âš ï¸ å…¨å±€åˆ†æå¤±è´¥: {e}")
        return {}


# ===== æœ¯è¯­æå–ä¸ç®¡ç†ï¼ˆå¢å¼ºç‰ˆï¼‰=====
class TerminologyManager:
    def __init__(self):
        self.terms = {}  # ä¸»æœ¯è¯­è¡¨
        self.domain_terms = {}  # æŒ‰é¢†åŸŸåˆ†ç±»çš„æœ¯è¯­
        self.term_priority = {}  # æœ¯è¯­ä¼˜å…ˆçº§ï¼ˆ0-100ï¼Œé»˜è®¤50ï¼‰
        
        # æ‰©å±•é¢†åŸŸå…³é”®è¯
        self.domain_keywords = {
            "AI": ["transformer", "attention", "neural network", "GPT", "LLM", 
                   "embedding", "tokenizer", "fine-tuning", "reinforcement learning",
                   "deep learning", "machine learning", "model", "prompt", "inference",
                   "training", "dataset", "hyperparameter", "accuracy", "precision"],
            "Math": ["derivative", "integral", "matrix", "vector", "function",
                    "equation", "theorem", "proof", "optimization", "calculus",
                    "algebra", "geometry", "statistics", "probability", "algorithm"],
            "Programming": ["API", "function", "variable", "algorithm", "database",
                          "framework", "compiler", "debugger", "deployment",
                          "code", "syntax", "semantics", "runtime", "memory",
                          "performance", "security", "version", "repository"],
            "Travel": [
                "destination", "travel", "visit", "tour", "trip", "journey", "vacation",
                "backpack", "explore", "adventure", "culture", "heritage", "UNESCO",
                "landmark", "beach", "mountain", "resort", "itinerary", "passport",
                "flight", "hotel", "restaurant", "local", "attraction", "guide"
            ],
            "Science": ["experiment", "hypothesis", "theory", "research", "discovery",
                        "observation", "data", "analysis", "conclusion", "evidence",
                        "methodology", "variable", "control", "sample", "result"],
            "Technology": ["device", "software", "hardware", "system", "network",
                          "internet", "computer", "smartphone", "application", "interface",
                          "user experience", "design", "development", "innovation", "trend"],
            "Education": ["learning", "teaching", "student", "teacher", "curriculum",
                         "course", "lesson", "assessment", "examination", "grade",
                         "knowledge", "skill", "competency", "pedagogy", "method"],
            "Business": ["market", "economy", "finance", "investment", "profit",
                        "loss", "revenue", "cost", "strategy", "management",
                        "entrepreneur", "company", "industry", "competition", "customer"]
        }
    
    def detect_domains(self, text: str) -> List[str]:
        """
        æ£€æµ‹æ–‡æœ¬æ‰€å±é¢†åŸŸï¼Œè¿”å›ç½®ä¿¡åº¦æœ€é«˜çš„å‰2ä¸ªé¢†åŸŸ
        """
        detected = []
        text_lower = text.lower()
        domain_scores = {}
        
        # è®¡ç®—æ¯ä¸ªé¢†åŸŸçš„åŒ¹é…å¾—åˆ†
        for domain, keywords in self.domain_keywords.items():
            score = sum(1 for kw in keywords if kw.lower() in text_lower)
            if score > 0:
                domain_scores[domain] = score
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œè¿”å›å‰2ä¸ªé¢†åŸŸ
        sorted_domains = sorted(domain_scores.items(), key=lambda x: x[1], reverse=True)
        detected = [domain for domain, score in sorted_domains[:2]]
        
        return detected or ["é€šç”¨"]
    
    def extract_terms(self, text: str, target_language: str = 'ç®€ä½“ä¸­æ–‡') -> Dict[str, str]:
        """
        å¢å¼ºçš„æœ¯è¯­æå–åŠŸèƒ½
        """
        domains = self.detect_domains(text)
        logger.info(f"ğŸ” æ£€æµ‹åˆ°é¢†åŸŸ: {domains}")
        
        domain_desc = "ã€".join(domains)
        prompt = f"""ä½ æ˜¯ä¸“ä¸šæœ¯è¯­æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹{domain_desc}é¢†åŸŸçš„æ–‡æœ¬ä¸­æå–å…³é”®æœ¯è¯­ã€‚

è¦æ±‚ï¼š
1. æå–æŠ€æœ¯æœ¯è¯­ã€ä¸“æœ‰åè¯ã€å…³é”®æ¦‚å¿µã€äººåã€åœ°åã€ç»„ç»‡å
2. æ¯ä¸ªæœ¯è¯­æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„{target_language}ç¿»è¯‘
3. æœ¯è¯­ç¿»è¯‘éœ€ä¿æŒä¸€è‡´æ€§å’Œå‡†ç¡®æ€§
4. è¾“å‡ºæ ‡å‡†JSONæ ¼å¼
5. ä¸è¦åŒ…å«è¿‡äºé€šç”¨çš„è¯æ±‡
6. æä¾›5-20ä¸ªæœ€å…³é”®çš„æœ¯è¯­

æ–‡æœ¬ï¼ˆèŠ‚é€‰ï¼‰:
{text[:2000]}

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
  "transformer": "Transformeræ¨¡å‹",
  "attention mechanism": "æ³¨æ„åŠ›æœºåˆ¶",
  "OpenAI": "OpenAI",
  "GPT-4": "GPT-4"
}}"""

        try:
            client = get_client()
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šæœ¯è¯­æå–ä¸“å®¶ï¼Œç²¾é€šå¤šé¢†åŸŸæœ¯è¯­ç¿»è¯‘ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å’Œç¿»è¯‘å„ç§ä¸“æœ‰åè¯å’ŒæŠ€æœ¯æœ¯è¯­ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # é™ä½æ¸©åº¦ï¼Œæé«˜æœ¯è¯­å‡†ç¡®æ€§
                max_tokens=1000  # å¢åŠ æœ€å¤§tokenæ•°
            )
            
            raw = response.choices[0].message.content.strip()
            logger.debug(f"æœ¯è¯­æå–ç»“æœ: {raw[:200]}...")

            try:
                terms = json.loads(raw)
            except json.JSONDecodeError:
                terms = {}
                # æ›´å¥å£®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
                matches = re.findall(r'"([^"]+)"\s*:\s*"([^"]+)"', raw)
                for en, zh in matches:
                    if en and zh and len(en) > 1:  # è¿‡æ»¤è¿‡çŸ­çš„æœ¯è¯­
                        terms[en] = zh

            # è¿‡æ»¤æ— æ•ˆæœ¯è¯­
            filtered_terms = {}
            for term, translation in terms.items():
                # è¿‡æ»¤è¿‡äºé€šç”¨çš„è¯æ±‡
                if len(term) > 1 and not term.lower() in ["the", "and", "or", "but", "in", "on", "at", "to", "for", "with"]:
                    filtered_terms[term] = translation
            
            # æ›´æ–°æœ¯è¯­è¡¨
            self.terms.update(filtered_terms)
            
            # æŒ‰é¢†åŸŸå­˜å‚¨æœ¯è¯­
            for domain in domains:
                if domain not in self.domain_terms:
                    self.domain_terms[domain] = {}
                self.domain_terms[domain].update(filtered_terms)
            
            logger.info(f"âœ… æå–åˆ° {len(filtered_terms)} ä¸ªå…³é”®æœ¯è¯­")
            return filtered_terms
            
        except Exception as e:
            logger.warning(f"âš ï¸ æœ¯è¯­æå–å¤±è´¥: {e}")
            return {}
    
    def apply_terms(self, text: str) -> str:
        """
        å¢å¼ºçš„æœ¯è¯­åº”ç”¨æœºåˆ¶
        1. æŒ‰æœ¯è¯­é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿æœ¯è¯­
        2. è€ƒè™‘æœ¯è¯­ä¸Šä¸‹æ–‡
        3. æé«˜åŒ¹é…å‡†ç¡®æ€§
        """
        if not self.terms:
            return text
        
        # æŒ‰æœ¯è¯­é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿æœ¯è¯­
        sorted_terms = sorted(self.terms.items(), key=lambda x: len(x[0]), reverse=True)
        
        # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼Œè€ƒè™‘å•è¯è¾¹ç•Œ
        for en, zh in sorted_terms:
            if not en or not zh:
                continue
            
            # æ„å»ºæ›´å¥å£®çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œè€ƒè™‘å¤§å°å†™å’Œå•è¯è¾¹ç•Œ
            pattern = r'\b' + re.escape(en) + r'\b'
            
            # ä½¿ç”¨re.IGNORECASEè¿›è¡Œå¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
            text = re.sub(pattern, zh, text, flags=re.IGNORECASE)
        
        return text
    
    def add_custom_term(self, term: str, translation: str, priority: int = 50, domain: str = "é€šç”¨"):
        """
        æ·»åŠ è‡ªå®šä¹‰æœ¯è¯­
        """
        self.terms[term] = translation
        self.term_priority[term] = priority
        
        if domain not in self.domain_terms:
            self.domain_terms[domain] = {}
        self.domain_terms[domain][term] = translation
    
    def get_terms_by_domain(self, domain: str) -> Dict[str, str]:
        """
        è·å–ç‰¹å®šé¢†åŸŸçš„æœ¯è¯­
        """
        return self.domain_terms.get(domain, {})


# ===== é«˜çº§ç¿»è¯‘å™¨ =====
class AdvancedTranslator:
    def __init__(self, model_name: str = MODEL_NAME):
        self.model_name = model_name
        self.client = get_client()
        self.term_manager = TerminologyManager()
        
        self.few_shot_examples = [
            {
                "source": "So basically what we're doing here is taking the derivative of the loss function.",
                "target": "æ‰€ä»¥åŸºæœ¬ä¸Šæˆ‘ä»¬åœ¨è¿™é‡Œåšçš„å°±æ˜¯è®¡ç®—æŸå¤±å‡½æ•°çš„å¯¼æ•°ã€‚",
                "note": "ä¿ç•™æ‰€ä»¥ã€åŸºæœ¬ä¸Šç­‰å£è¯­åŒ–è¡¨è¾¾ï¼Œæ•°å­¦æœ¯è¯­å‡†ç¡®ç¿»è¯‘"
            },
            {
                "source": "This is a really cool technique that allows us to...",
                "target": "è¿™æ˜¯ä¸€ä¸ªéå¸¸é…·çš„æŠ€æœ¯ï¼Œå®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿâ€¦â€¦",
                "note": "\"really cool\" ç¿»è¯‘ä¸º éå¸¸é…· è€Œé çœŸçš„å¾ˆé…·ï¼Œä¿æŒå£è¯­åŒ–é£æ ¼"
            },
            {
                "source": "Now, you might be wondering why we use attention here.",
                "target": "ç°åœ¨ï¼Œä½ å¯èƒ½ä¼šæƒ³çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ã€‚",
                "note": "ä¿ç•™ ä½ å¯èƒ½ä¼šæƒ³ ç­‰å¯¹è¯æ„Ÿï¼ŒæŠ€æœ¯æœ¯è¯­å‡†ç¡®ç¿»è¯‘"
            },
            {
                "source": "The transformer architecture has revolutionized natural language processing.",
                "target": "Transformeræ¶æ„å·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚",
                "note": "æŠ€æœ¯æœ¯è¯­å‡†ç¡®ç¿»è¯‘ï¼Œä¿æŒå¥å­æµç•…æ€§"
            },
            {
                "source": "In conclusion, this study demonstrates the effectiveness of our approach.",
                "target": "æ€»ä¹‹ï¼Œè¿™é¡¹ç ”ç©¶è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
                "note": "å­¦æœ¯è®ºæ–‡é£æ ¼ï¼Œä½¿ç”¨æ­£å¼ä½†æµç•…çš„è¡¨è¾¾"
            },
            {
                "source": "Let's take a closer look at the results from our experiment.",
                "target": "è®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹æˆ‘ä»¬å®éªŒçš„ç»“æœã€‚",
                "note": "ä½¿ç”¨ç¥ˆä½¿å¥ï¼Œä¿æŒäº²å’ŒåŠ›"
            },
            {
                "source": "The model achieved an accuracy of 95.2% on the test dataset.",
                "target": "è¯¥æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¾¾åˆ°äº†95.2%çš„å‡†ç¡®ç‡ã€‚",
                "note": "æ•°æ®å’Œç™¾åˆ†æ¯”çš„å‡†ç¡®è¡¨è¾¾"
            }
        ]
    
    def build_translation_prompt(self, text, context_prev, context_next, terms, target_duration, target_language='ç®€ä½“ä¸­æ–‡', context_prev_translations=None):
        max_chars = int(target_duration * 4.5)
        term_list = "\n".join([f"- {en} â†’ {zh}" for en, zh in list(terms.items())[:20]])  # å¢åŠ æœ¯è¯­åˆ—è¡¨é•¿åº¦
        examples = "\n\n".join([
            f"åŸæ–‡: {ex['source']}\nè¯‘æ–‡: {ex['target']}\næ³¨æ„: {ex['note']}"
            for ex in self.few_shot_examples[:3]  # å¢åŠ ç¤ºä¾‹æ•°é‡
        ])
        
        context = []
        for i, t in enumerate(context_prev, 1):
            if t: context.append(f"å‰{i}å¥åŸæ–‡: {t}")
        
        # æ·»åŠ å‰ä¸€å¥çš„ç¿»è¯‘ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¢å¼ºè¿è´¯æ€§
        if context_prev_translations:
            for i, t in enumerate(reversed(context_prev_translations), 1):
                if t: context.append(f"å‰{i}å¥è¯‘æ–‡: {t}")
        
        context.append(f"ã€å½“å‰å¥åŸæ–‡ã€‘: {text}")
        
        for i, t in enumerate(context_next, 1):
            if t: context.append(f"å{i}å¥åŸæ–‡: {t}")
        
        context_str = "\n".join(context)
        
        term_display = term_list if term_list else "æ— ç‰¹å®šæœ¯è¯­"
        return f"""ä½ æ˜¯ä¸“ä¸šè§†é¢‘å­—å¹•ç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†è‹±æ–‡è§†é¢‘ç¿»è¯‘æˆåœ°é“ã€å£è¯­åŒ–çš„{target_language}ã€‚

# ç¿»è¯‘åŸåˆ™
1. **è‡ªç„¶æµç•…**: ç¬¦åˆ{target_language}è¡¨è¾¾ä¹ æƒ¯ï¼Œä¸è¦é€å­—ç›´è¯‘
2. **å£è¯­åŒ–**: ä¿ç•™"æ‰€ä»¥"ã€"å…¶å®"ã€"é‚£ä¹ˆ"ç­‰è¯­æ°”è¯
3. **å‡†ç¡®æ€§**: ä¸¥æ ¼ä½¿ç”¨æœ¯è¯­è¡¨ï¼Œä¿æŒå…¨æ–‡ä¸€è‡´
4. **ä¸Šä¸‹æ–‡è¿è´¯**: å‚è€ƒå‰åæ–‡ï¼Œç¡®ä¿ç¿»è¯‘è¿è´¯è‡ªç„¶
5. **é¢†åŸŸé€‚é…**: æ ¹æ®å†…å®¹è°ƒæ•´ç¿»è¯‘é£æ ¼ï¼ˆå¦‚ç§‘æŠ€ã€æ•™è‚²ã€å¨±ä¹ç­‰ï¼‰
6. **æ—¶é•¿åŒ¹é…**: è¯‘æ–‡çº¦{target_duration:.1f}ç§’ï¼Œæœ€å¤š{max_chars}ä¸ªæ±‰å­—

# æœ¯è¯­è¡¨ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
{term_display}

# ç¿»è¯‘ç¤ºä¾‹ï¼ˆå­¦ä¹ é£æ ¼ï¼‰
{examples}

# å¾…ç¿»è¯‘å†…å®¹ï¼ˆå«ä¸Šä¸‹æ–‡ï¼‰
{context_str}

# è¾“å‡ºè¦æ±‚
ä¸¥æ ¼è¾“å‡ºJSONæ ¼å¼: {{"translation": "è¯‘æ–‡"}}
åªç¿»è¯‘ã€å½“å‰å¥åŸæ–‡ã€‘ï¼Œä¸è¦ç¿»è¯‘ä¸Šä¸‹æ–‡ï¼"""
    
    def translate_first_pass(self, text, context_prev, context_next, terms, target_duration, target_language='ç®€ä½“ä¸­æ–‡', context_prev_translations=None):
        prompt = self.build_translation_prompt(text, context_prev, context_next, terms, target_duration, target_language, context_prev_translations)
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šè§†é¢‘ç¿»è¯‘ä¸“å®¶ã€‚ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä½¿ç”¨å£è¯­åŒ–è¡¨è¾¾ï¼Œä¿æŒæœ¯è¯­ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,  # é™ä½æ¸©åº¦ï¼Œæé«˜ä¸€è‡´æ€§
                top_p=0.8,  # é™ä½top_pï¼Œå‡å°‘éšæœºæ€§
                max_tokens=300  # å¢åŠ æœ€å¤§tokenæ•°
            )
            raw = response.choices[0].message.content.strip()
            try:
                data = json.loads(raw)
                return str(data.get('translation', '')).strip()
            except:
                match = re.search(r'"translation"\s*:\s*"((?:[^"\\]|\\.)*)"', raw)
                return match.group(1).replace('\\"', '"').replace('\\', '') if match else ""
        except Exception as e:
            logger.warning(f"âš ï¸ ç¬¬ä¸€éç¿»è¯‘å¤±è´¥: {e}")
            return ""
    
    def refine_translation(self, original, first_translation, target_duration, target_language='ç®€ä½“ä¸­æ–‡', context_prev=None, context_next=None):
        if not first_translation:
            return original
        max_chars = int(target_duration * 4.5)
        current_chars = len(first_translation)
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = ""
        if context_prev:
            context_info += f"å‰å¥åŸæ–‡: {context_prev[-1]}\n"
        if context_next:
            context_info += f"åå¥åŸæ–‡: {context_next[0]}\n"
        
        prompt = f"""ä½ æ˜¯ç¿»è¯‘è´¨é‡å®¡æ ¡ä¸“å®¶ã€‚è¯·ä¼˜åŒ–ä»¥ä¸‹ç¿»è¯‘ï¼Œä½¿å…¶æ›´åŠ è‡ªç„¶æµç•…ã€‚

{context_info}
åŸæ–‡: {original}

åˆè¯‘: {first_translation}

ä¼˜åŒ–è¦æ±‚:
1. ä¿æŒåŸæ„ä¸å˜
2. æ›´åŠ å£è¯­åŒ–ã€è‡ªç„¶
3. ä¸ä¸Šä¸‹æ–‡ä¿æŒè¿è´¯
4. é•¿åº¦æ§åˆ¶åœ¨ {max_chars} ä¸ªæ±‰å­—å†…ï¼ˆå½“å‰ {current_chars} å­—ï¼‰
5. å»é™¤å†—ä½™ï¼Œä½¿ç”¨æ›´ç®€æ´çš„è¡¨è¾¾
6. ç¡®ä¿ä¸“ä¸šæœ¯è¯­ä½¿ç”¨æ­£ç¡®

è¾“å‡ºJSON: {{"refined": "ä¼˜åŒ–åçš„è¯‘æ–‡"}}"""
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šç¿»è¯‘å®¡æ ¡ä¸“å®¶ã€‚ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¼˜åŒ–ç¿»è¯‘ä½¿å…¶æ›´åŠ è‡ªç„¶ã€è¿è´¯ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4,
                max_tokens=250
            )
            raw = response.choices[0].message.content.strip()
            try:
                data = json.loads(raw)
                refined = data.get('refined', first_translation)
            except:
                match = re.search(r'"refined"\s*:\s*"((?:[^"\\]|\\.)*)"', raw)
                refined = match.group(1) if match else first_translation
            return refined.replace('\\"', '"').replace('\\', '')
        except Exception as e:
            logger.warning(f"âš ï¸ ç¿»è¯‘ä¼˜åŒ–å¤±è´¥: {e}")
            return first_translation
    
    def translate_with_quality_check(self, text, context_prev, context_next, terms, target_duration, target_language='ç®€ä½“ä¸­æ–‡', context_prev_translations=None):
        first_pass = self.translate_first_pass(text, context_prev, context_next, terms, target_duration, target_language, context_prev_translations)
        if not first_pass:
            return text, False
        refined = self.refine_translation(text, first_pass, target_duration, target_language, context_prev, context_next)
        final = self.term_manager.apply_terms(refined)
        is_good = self._quality_check(text, final, target_duration)
        return final, is_good
    
    def _quality_check(self, source, target, target_duration):
        """
        å¢å¼ºçš„è´¨é‡æ£€æŸ¥æœºåˆ¶ï¼Œå¢åŠ æ›´å¤šè´¨é‡æ£€æŸ¥ç»´åº¦
        """
        # åŸºæœ¬æ£€æŸ¥
        if not target or len(target) < 3:
            logger.warning(f"âš ï¸ ç¿»è¯‘è¿‡çŸ­: {target}")
            return False
        
        # é•¿åº¦æ£€æŸ¥ï¼šç¡®ä¿ç¿»è¯‘é•¿åº¦ä¸éŸ³é¢‘æ—¶é•¿åŒ¹é…
        max_chars = int(target_duration * 5.0)
        min_chars = max(1, int(target_duration * 0.5))
        if len(target) > max_chars:
            logger.warning(f"âš ï¸ ç¿»è¯‘è¿‡é•¿: {len(target)}/{max_chars}å­—ç¬¦")
            return False
        if len(target) < min_chars:
            logger.warning(f"âš ï¸ ç¿»è¯‘è¿‡çŸ­: {len(target)}/{min_chars}å­—ç¬¦")
            return False
        
        # è‹±æ–‡æ¯”ä¾‹æ£€æŸ¥ï¼šç¡®ä¿ç¿»è¯‘ä»¥ä¸­æ–‡ä¸ºä¸»
        english_chars = sum(1 for c in target if c.isalpha() and ord(c) < 128)
        if english_chars > len(target) * 0.4:
            logger.warning(f"âš ï¸ è‹±æ–‡æ¯”ä¾‹è¿‡é«˜: {english_chars}/{len(target)}å­—ç¬¦")
            return False
        
        # è¯­æ³•æ£€æŸ¥ï¼šé¿å…æ˜æ˜¾çš„è¯­æ³•é”™è¯¯
        if target.count('ã€‚') > 3:  # é¿å…è¿‡å¤šå¥å­
            logger.warning(f"âš ï¸ å¥å­è¿‡å¤š: {target}")
            return False
        
        # æ£€æŸ¥åŸºæœ¬æ ‡ç‚¹ç¬¦å·ä½¿ç”¨
        if target and target[-1] not in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', 'ï¼›', 'ï¼š', 'â€', '']:
            logger.warning(f"âš ï¸ ç¼ºå°‘å¥æœ«æ ‡ç‚¹: {target}")
            return False
        
        # æ£€æŸ¥è¿‡åº¦é‡å¤
        if len(target) > 10:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­é‡å¤çš„å­—ç¬¦
            for i in range(len(target) - 2):
                if target[i] == target[i+1] == target[i+2]:
                    logger.warning(f"âš ï¸ è¿‡åº¦é‡å¤: {target}")
                    return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„æ¼è¯‘
        source_words = len(source.split())
        target_words = len(target)
        if source_words > 10 and target_words < source_words * 0.3:
            logger.warning(f"âš ï¸ å¯èƒ½æ¼è¯‘: åŸæ–‡{source_words}è¯ï¼Œè¯‘æ–‡{target_words}å­—")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„è¿‡åº¦ç¿»è¯‘
        if target_words > source_words * 3:
            logger.warning(f"âš ï¸ å¯èƒ½è¿‡åº¦ç¿»è¯‘: åŸæ–‡{source_words}è¯ï¼Œè¯‘æ–‡{target_words}å­—")
            return False
        
        # æ£€æŸ¥æœ¯è¯­ä¸€è‡´æ€§ï¼ˆç®€å•æ£€æŸ¥ï¼šç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæœ¯è¯­è¢«æ­£ç¡®ç¿»è¯‘ï¼‰
        if self.term_manager.terms and any(term.lower() in source.lower() for term in self.term_manager.terms):
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ¯è¯­è¢«æ­£ç¡®ç¿»è¯‘
            translated_terms = sum(1 for term, trans in self.term_manager.terms.items() if trans in target)
            if translated_terms == 0:
                logger.warning(f"âš ï¸ æœ¯è¯­æœªè¢«æ­£ç¡®ç¿»è¯‘: {source} â†’ {target}")
                # ä¸æ˜¯è‡´å‘½é”™è¯¯ï¼Œç»§ç»­æ£€æŸ¥å…¶ä»–ç»´åº¦
        
        return True


# ===== æ–°å¢ï¼šè‡ªåŠ¨æå–å…‹éš†å‚è€ƒéŸ³é¢‘ =====
def extract_speaker_clips(folder: str, max_duration: float = 30.0):
    """
    ä¸ºæ¯ä¸ªè¯´è¯äººæå–ä¸€æ®µ <= max_duration ç§’çš„å¹²å‡€è¯­éŸ³ï¼Œç”¨äº TTS å…‹éš†
    ä¿å­˜ä¸º SPEAKER/SPEAKER_XX_CLONE.wav
    """
    if not HAS_LIBROSA:
        logger.warning("âš ï¸ æœªå®‰è£… librosaï¼Œè·³è¿‡å…‹éš†éŸ³é¢‘æå–")
        return

    transcript_path = os.path.join(folder, 'translation.json')
    vocals_path = os.path.join(folder, 'audio_vocals.wav')
    speaker_dir = os.path.join(folder, 'SPEAKER')
    
    if not os.path.exists(transcript_path) or not os.path.exists(vocals_path):
        logger.warning("âš ï¸ ç¼ºå°‘ translation.json æˆ– audio_vocals.wavï¼Œè·³è¿‡å…‹éš†éŸ³é¢‘æå–")
        return
    
    if not os.path.exists(speaker_dir):
        os.makedirs(speaker_dir)

    # åŠ è½½äººå£°éŸ³é¢‘
    try:
        vocals, sr = librosa.load(vocals_path, sr=16000)
    except Exception as e:
        logger.error(f"âŒ æ— æ³•åŠ è½½äººå£°éŸ³é¢‘: {e}")
        return

    # æŒ‰è¯´è¯äººåˆ†ç»„ç‰‡æ®µï¼ˆé€‰æ‹©æœ€é•¿ä¸” <= max_duration çš„ï¼‰
    with open(transcript_path, 'r', encoding='utf-8') as f:
        transcript = json.load(f)
    
    speaker_segments = {}
    for line in transcript:
        speaker = line.get('speaker', 'SPEAKER_00')
        start = float(line.get('start', 0))
        end = float(line.get('end', 0))
        text = line.get('text', '').strip()
        
        if not text or end - start <= 0.5:  # å¿½ç•¥å¤ªçŸ­æˆ–ç©ºæ–‡æœ¬
            continue
        
        duration = end - start
        if duration > max_duration:  # è¶…é•¿åˆ™è·³è¿‡ï¼ˆæˆ–å¯è£å‰ªï¼Œä½†ç®€å•èµ·è§è·³è¿‡ï¼‰
            continue
        
        if speaker not in speaker_segments or duration > speaker_segments[speaker]['duration']:
            speaker_segments[speaker] = {
                'start': start,
                'end': end,
                'text': text,
                'duration': duration
            }
    
    # ä¿å­˜æ¯ä¸ªè¯´è¯äººçš„æœ€ä½³ç‰‡æ®µ
    for speaker, seg in speaker_segments.items():
        start_sample = int(seg['start'] * sr)
        end_sample = int(seg['end'] * sr)
        clip = vocals[start_sample:end_sample]
        
        if len(clip) == 0:
            continue
        
        output_path = os.path.join(speaker_dir, f"{speaker}_CLONE.wav")
        try:
            import soundfile as sf
            sf.write(output_path, clip, sr)
            logger.info(f"ğŸ”Š ä¿å­˜å…‹éš†éŸ³é¢‘: {output_path} ({seg['duration']:.1f}s)")
            
            # åŒæ—¶ä¿å­˜æ–‡æœ¬ï¼ˆç”¨äº VoxCPM çš„ prompt_textï¼‰
            txt_path = output_path.replace('.wav', '.txt')
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(seg['text'])
            logger.info(f"ğŸ“„ ä¿å­˜å…‹éš†æ–‡æœ¬: {txt_path}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")


# ===== ä¸»ç¿»è¯‘å‡½æ•° =====
def translate_advanced(folder: str, target_language: str = 'ç®€ä½“ä¸­æ–‡') -> bool:
    translation_path = os.path.join(folder, 'translation.json')
    if os.path.exists(translation_path):
        logger.info(f"âœ… ç¿»è¯‘å·²å­˜åœ¨: {folder}")
        return True
    
    transcript_path = os.path.join(folder, 'transcript.json')
    if not os.path.exists(transcript_path):
        logger.error(f"âŒ å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {transcript_path}")
        return False
    
    with open(transcript_path, 'r', encoding='utf-8') as f:
        transcript = json.load(f)
    logger.info(f"ğŸ“„ åŠ è½½äº† {len(transcript)} æ¡å­—å¹•")
    
    # è·å–è§†é¢‘é¢†åŸŸä¿¡æ¯
    full_text = ' '.join(line.get('text', '') for line in transcript[:100])  # ä½¿ç”¨å‰100æ¡å­—å¹•æ£€æµ‹é¢†åŸŸ
    
    # åˆå§‹åŒ–ç¿»è¯‘å™¨å¹¶æ£€æµ‹é¢†åŸŸ
    translator = AdvancedTranslator()
    domains = translator.term_manager.detect_domains(full_text)
    logger.info(f"ğŸŒ è§†é¢‘é¢†åŸŸ: {domains}")
    
    # æå–æœ¯è¯­
    global_terms = analyze_transcript(transcript, target_language)
    domain_terms = translator.term_manager.extract_terms(full_text, target_language)
    all_terms = {**domain_terms, **global_terms}
    translator.term_manager.terms = all_terms
    
    terms_path = os.path.join(folder, 'terminology.json')
    with open(terms_path, 'w', encoding='utf-8') as f:
        json.dump(all_terms, f, indent=2, ensure_ascii=False)
    
    translations = []
    quality_flags = []
    
    for i, line in enumerate(transcript):
        text = line.get('text', '').strip()
        if not text:
            translations.append("")
            quality_flags.append(False)
            continue
        
        # å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ï¼šæ‰©å¤§ä¸Šä¸‹æ–‡çª—å£
        context_prev = [transcript[j].get('text', '') for j in range(max(0, i-3), i)]  # å‰3å¥
        context_next = [transcript[j].get('text', '') for j in range(i+1, min(len(transcript), i+4))]  # å3å¥
        
        # æ·»åŠ å‰ä¸€å¥çš„ç¿»è¯‘ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¢å¼ºè¿è´¯æ€§
        context_prev_translations = []
        for j in range(max(0, i-3), i):
            if j < len(translations) and translations[j]:
                context_prev_translations.append(translations[j])
        
        start = float(line.get('start', 0))
        end = float(line.get('end', 0))
        vad_duration = line.get('vad_duration')
        target_duration = min(float(vad_duration), end - start) if vad_duration else (end - start)
        
        progress = (i + 1) / len(transcript) * 100
        logger.info(f"ğŸ“ˆ [{i+1}/{len(transcript)}] ({progress:.1f}%) - {text[:50]}...")
        
        translation, is_good = translator.translate_with_quality_check(
            text, context_prev, context_next, all_terms,
            target_duration, target_language, context_prev_translations
        )
        
        translations.append(translation)
        quality_flags.append(is_good)
        
        logger.info(f"ğŸ’¬ è¯‘æ–‡: {translation}")
        logger.info(f"     è´¨é‡: {'âœ…' if is_good else 'âš ï¸ '} | æ—¶é•¿: {target_duration:.1f}s")
        logger.info("-" * 60)
        time.sleep(0.2)
    
    result = []
    for i, line in enumerate(transcript):
        result.append({
            "start": float(line.get('start', 0)),
            "end": float(line.get('end', 0)),
            "text": line.get('text', ''),
            "speaker": line.get('speaker', ''),
            "translation": translations[i]
        })
    
    with open(translation_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    total = len(quality_flags)
    success = sum(quality_flags)
    stats = {'total': total, 'success': success, 'success_rate': round(100 * success / total, 2) if total else 0}
    stats_path = os.path.join(folder, 'translation_stats.json')
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    logger.success(f"âœ… ç¿»è¯‘å®Œæˆ: {translation_path}")
    logger.info(f"ğŸ“Š æˆåŠŸç‡: {stats['success_rate']:.1f}% ({success}/{total})")
    
    # === æ–°å¢ï¼šè‡ªåŠ¨æå–å…‹éš†éŸ³é¢‘ ===
    logger.info("âœ‚ï¸ æ­£åœ¨æå–è¯´è¯äººå…‹éš†éŸ³é¢‘...")
    extract_speaker_clips(folder, max_duration=10.0)
    
    return True


def translate_all_advanced(root_folder: str, target_language: str = 'ç®€ä½“ä¸­æ–‡') -> int:
    folders = [
        root for root, _, files in os.walk(root_folder)
        if 'transcript.json' in files and 'translation.json' not in files
    ]
    logger.info(f"ğŸ¯ æ‰¾åˆ° {len(folders)} ä¸ªå¾…ç¿»è¯‘è§†é¢‘")
    success_count = 0
    for i, folder in enumerate(folders, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¬ å¤„ç† ({i}/{len(folders)}): {folder}")
        if translate_advanced(folder, target_language):
            success_count += 1
        if i < len(folders):
            time.sleep(2)
    logger.success(f"ğŸ å®Œæˆ! æˆåŠŸç¿»è¯‘ {success_count}/{len(folders)} ä¸ªè§†é¢‘")
    return success_count


if __name__ == '__main__':
    logger.remove()
    logger.add(
        sys.stderr,
        level="INFO",
        format="<green>{time:MM-DD HH:mm:ss}</green> | <level>{level: <6}</level> | <cyan>{message}</cyan>"
    )
    translate_all_advanced('videos', 'ç®€ä½“ä¸­æ–‡')