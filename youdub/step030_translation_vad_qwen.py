#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§è§†é¢‘å­—å¹•ç¿»è¯‘æ¨¡å—ï¼ˆQwen2.5 + å¤šè½®æ ¡å¯¹ + æ™ºèƒ½æœ¯è¯­åº“ + è‡ªåŠ¨å…‹éš†éŸ³é¢‘æå–ï¼‰

æ–°å¢åŠŸèƒ½ï¼š
âœ… è‡ªåŠ¨ä» audio_vocals.wav æå–æ¯ä¸ªè¯´è¯äºº â‰¤10 ç§’çš„å¹²å‡€è¯­éŸ³ç‰‡æ®µ
âœ… ä¿å­˜ä¸º SPEAKER/SPEAKER_XX_CLONE.wav ä¾› VoxCPM ä½¿ç”¨

ä½œè€…: Advanced Translation Team
æ—¥æœŸ: 2026-01-04
ç‰ˆæœ¬: 2.1
"""

import json
import os
import re
import sys
import time
from typing import List, Dict, Tuple, Optional
from openai import OpenAI
from loguru import logger
from dotenv import load_dotenv

# å°è¯•å¯¼å…¥éŸ³é¢‘å¤„ç†åº“ï¼ˆç”¨äºè‡ªåŠ¨æå–å…‹éš†éŸ³é¢‘ï¼‰
try:
    import librosa
    import numpy as np
    HAS_LIBROSA = True
except ImportError:
    HAS_LIBROSA = False
    logger.warning("âš ï¸ librosa æœªå®‰è£…ï¼Œå°†è·³è¿‡è‡ªåŠ¨å…‹éš†éŸ³é¢‘æå–")

load_dotenv()

# ===== é…ç½® =====
MODEL_NAME = os.getenv('MODEL_NAME', 'qwen2.5:14b')
API_BASE = os.getenv('OPENAI_API_BASE', 'http://127.0.0.1:11434/v1')
API_KEY = os.getenv('OPENAI_API_KEY', 'ollama')

logger.info(f"ğŸ¤– ä½¿ç”¨ç¿»è¯‘æ¨¡å‹: {MODEL_NAME}")
logger.info(f"ğŸŒ APIåœ°å€: {API_BASE}")

_client: Optional[OpenAI] = None

def get_client() -> OpenAI:
    global _client
    if _client is None:
        _client = OpenAI(base_url=API_BASE, api_key=API_KEY, timeout=120.0)
    return _client


# ===== å…¨å±€åˆ†æå™¨ï¼ˆæ–°å¢ï¼‰=====
def analyze_transcript(transcript: List[Dict], target_language: str = 'ç®€ä½“ä¸­æ–‡') -> Dict[str, str]:
    full_text = ' '.join(line.get('text', '') for line in transcript[:50])
    input_text = full_text[:3000]

    prompt = f"""ä½ æ˜¯ä¸“ä¸šè§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚è¯·ä»ä»¥ä¸‹å­—å¹•ä¸­æå–å…³é”®æœ¯è¯­ï¼ˆè‹±æ–‡ â†’ {target_language}ï¼‰ã€‚

è¦æ±‚ï¼š
1. æå–ä¸“æœ‰åè¯ã€åœ°åã€äººåã€æ–‡åŒ–æ¦‚å¿µã€æŠ€æœ¯æœ¯è¯­ç­‰
2. è¾“å‡ºæ ‡å‡†JSONæ ¼å¼

å­—å¹•å†…å®¹:
{input_text}

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
  "Paris": "å·´é»",
  "UNESCO": "è”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡",
  "Northern Lights": "åŒ—æå…‰"
}}"""

    try:
        client = get_client()
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿æå–å¤šé¢†åŸŸæœ¯è¯­ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=600
        )
        
        raw = response.choices[0].message.content.strip()
        logger.debug(f"å…¨å±€åˆ†ææœ¯è¯­åŸå§‹è¾“å‡º: {raw[:200]}...")

        try:
            terms = json.loads(raw)
        except:
            terms = {}
            matches = re.findall(r'"([^"]+)"\s*:\s*"([^"]+)"', raw)
            for en, zh in matches:
                if en and zh:
                    terms[en] = zh

        logger.info(f"ğŸ§  å…¨å±€åˆ†ææå–åˆ° {len(terms)} ä¸ªæœ¯è¯­")
        return terms

    except Exception as e:
        logger.warning(f"âš ï¸ å…¨å±€åˆ†æå¤±è´¥: {e}")
        return {}


# ===== æœ¯è¯­æå–ä¸ç®¡ç†ï¼ˆå¢å¼ºç‰ˆï¼‰=====
class TerminologyManager:
    def __init__(self):
        self.terms = {}
        self.domain_keywords = {
            "AI": ["transformer", "attention", "neural network", "GPT", "LLM", 
                   "embedding", "tokenizer", "fine-tuning", "reinforcement learning"],
            "Math": ["derivative", "integral", "matrix", "vector", "function",
                    "equation", "theorem", "proof", "optimization"],
            "Programming": ["API", "function", "variable", "algorithm", "database",
                          "framework", "compiler", "debugger", "deployment"],
            "Travel": [
                "destination", "travel", "visit", "tour", "trip", "journey", "vacation",
                "backpack", "explore", "adventure", "culture", "heritage", "UNESCO",
                "landmark", "beach", "mountain", "resort", "itinerary", "passport"
            ]
        }
    
    def detect_domains(self, text: str) -> List[str]:
        detected = []
        text_lower = text.lower()
        for domain, keywords in self.domain_keywords.items():
            score = sum(1 for kw in keywords if kw.lower() in text_lower)
            if score >= 2:
                detected.append(domain)
        return detected or ["é€šç”¨"]
    
    def extract_terms(self, text: str, target_language: str = 'ç®€ä½“ä¸­æ–‡') -> Dict[str, str]:
        domains = self.detect_domains(text)
        logger.info(f"ğŸ” æ£€æµ‹åˆ°é¢†åŸŸ: {domains}")
        
        domain_desc = "ã€".join(domains)
        prompt = f"""ä½ æ˜¯ä¸“ä¸šæœ¯è¯­æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹{domain_desc}é¢†åŸŸçš„æ–‡æœ¬ä¸­æå–å…³é”®æœ¯è¯­ã€‚

è¦æ±‚ï¼š
1. æå–æŠ€æœ¯æœ¯è¯­ã€ä¸“æœ‰åè¯ã€å…³é”®æ¦‚å¿µ
2. æ¯ä¸ªæœ¯è¯­æä¾›å‡†ç¡®çš„{target_language}ç¿»è¯‘
3. è¾“å‡ºæ ‡å‡†JSONæ ¼å¼

æ–‡æœ¬ï¼ˆèŠ‚é€‰ï¼‰:
{text[:1500]}

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š
{{
  "transformer": "Transformeræ¨¡å‹",
  "attention mechanism": "æ³¨æ„åŠ›æœºåˆ¶"
}}"""

        try:
            client = get_client()
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šæœ¯è¯­æå–ä¸“å®¶ï¼Œç²¾é€šæŠ€æœ¯é¢†åŸŸç¿»è¯‘ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=800
            )
            
            raw = response.choices[0].message.content.strip()
            logger.debug(f"æœ¯è¯­æå–ç»“æœ: {raw[:200]}...")

            try:
                terms = json.loads(raw)
            except:
                terms = {}
                matches = re.findall(r'"([^"]+)"\s*:\s*"([^"]+)"', raw)
                for en, zh in matches:
                    if en and zh:
                        terms[en] = zh

            self.terms.update(terms)
            logger.info(f"âœ… æå–åˆ° {len(terms)} ä¸ªå…³é”®æœ¯è¯­")
            return terms
            
        except Exception as e:
            logger.warning(f"âš ï¸ æœ¯è¯­æå–å¤±è´¥: {e}")
            return {}
    
    def apply_terms(self, text: str) -> str:
        if not self.terms:
            return text
        sorted_terms = sorted(self.terms.items(), key=lambda x: len(x[0]), reverse=True)
        for en, zh in sorted_terms:
            if en and zh:
                pattern = r'\b' + re.escape(en) + r'\b'
                text = re.sub(pattern, zh, text, flags=re.IGNORECASE)
        return text


# ===== é«˜çº§ç¿»è¯‘å™¨ =====
class AdvancedTranslator:
    def __init__(self, model_name: str = MODEL_NAME):
        self.model_name = model_name
        self.client = get_client()
        self.term_manager = TerminologyManager()
        
        self.few_shot_examples = [
            {
                "source": "So basically what we're doing here is taking the derivative of the loss function.",
                "target": "æ‰€ä»¥åŸºæœ¬ä¸Šæˆ‘ä»¬åœ¨è¿™é‡Œåšçš„å°±æ˜¯è®¡ç®—æŸå¤±å‡½æ•°çš„å¯¼æ•°ã€‚",
                "note": "ä¿ç•™æ‰€ä»¥ã€åŸºæœ¬ä¸Šç­‰å£è¯­åŒ–è¡¨è¾¾"
            },
            {
                "source": "This is a really cool technique that allows us to...",
                "target": "è¿™æ˜¯ä¸€ä¸ªéå¸¸é…·çš„æŠ€æœ¯ï¼Œå®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿâ€¦â€¦",
                "note": "\"really cool\" ç¿»è¯‘ä¸º éå¸¸é…· è€Œé çœŸçš„å¾ˆé…· "
            },
            {
                "source": "Now, you might be wondering why we use attention here.",
                "target": "ç°åœ¨ï¼Œä½ å¯èƒ½ä¼šæƒ³çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ã€‚",
                "note": "ä¿ç•™ ä½ å¯èƒ½ä¼šæƒ³ ç­‰å¯¹è¯æ„Ÿ"
            }
        ]
    
    def build_translation_prompt(self, text, context_prev, context_next, terms, target_duration, target_language='ç®€ä½“ä¸­æ–‡'):
        max_chars = int(target_duration * 4.5)
        term_list = "\n".join([f"- {en} â†’ {zh}" for en, zh in list(terms.items())[:15]])
        examples = "\n\n".join([
            f"åŸæ–‡: {ex['source']}\nè¯‘æ–‡: {ex['target']}\næ³¨æ„: {ex['note']}"
            for ex in self.few_shot_examples[:2]
        ])
        
        context = []
        for i, t in enumerate(context_prev, 1):
            if t: context.append(f"å‰{i}å¥: {t}")
        context.append(f"ã€å½“å‰å¥ã€‘: {text}")
        for i, t in enumerate(context_next, 1):
            if t: context.append(f"å{i}å¥: {t}")
        context_str = "\n".join(context)
        
        term_display = term_list if term_list else "æ— ç‰¹å®šæœ¯è¯­"
        return f"""ä½ æ˜¯ä¸“ä¸šè§†é¢‘å­—å¹•ç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†è‹±æ–‡è§†é¢‘ç¿»è¯‘æˆåœ°é“ã€å£è¯­åŒ–çš„{target_language}ã€‚

# ç¿»è¯‘åŸåˆ™
1. **è‡ªç„¶æµç•…**: ç¬¦åˆ{target_language}è¡¨è¾¾ä¹ æƒ¯ï¼Œä¸è¦é€å­—ç›´è¯‘
2. **å£è¯­åŒ–**: ä¿ç•™"æ‰€ä»¥"ã€"å…¶å®"ã€"é‚£ä¹ˆ"ç­‰è¯­æ°”è¯
3. **å‡†ç¡®æ€§**: ä¸¥æ ¼ä½¿ç”¨æœ¯è¯­è¡¨ï¼Œä¿æŒå…¨æ–‡ä¸€è‡´
4. **æ—¶é•¿åŒ¹é…**: è¯‘æ–‡çº¦{target_duration:.1f}ç§’ï¼Œæœ€å¤š{max_chars}ä¸ªæ±‰å­—

# æœ¯è¯­è¡¨ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
{term_display}

# ç¿»è¯‘ç¤ºä¾‹ï¼ˆå­¦ä¹ é£æ ¼ï¼‰
{examples}

# å¾…ç¿»è¯‘å†…å®¹ï¼ˆå«ä¸Šä¸‹æ–‡ï¼‰
{context_str}

# è¾“å‡ºè¦æ±‚
ä¸¥æ ¼è¾“å‡ºJSONæ ¼å¼: {{"translation": "è¯‘æ–‡"}}
åªç¿»è¯‘ã€å½“å‰å¥ã€‘ï¼Œä¸è¦ç¿»è¯‘ä¸Šä¸‹æ–‡ï¼"""
    
    def translate_first_pass(self, text, context_prev, context_next, terms, target_duration, target_language='ç®€ä½“ä¸­æ–‡'):
        prompt = self.build_translation_prompt(text, context_prev, context_next, terms, target_duration, target_language)
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šè§†é¢‘ç¿»è¯‘ä¸“å®¶ã€‚ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä½¿ç”¨å£è¯­åŒ–è¡¨è¾¾ï¼Œä¿æŒæœ¯è¯­ä¸€è‡´æ€§ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                top_p=0.9,
                max_tokens=250
            )
            raw = response.choices[0].message.content.strip()
            try:
                data = json.loads(raw)
                return str(data.get('translation', '')).strip()
            except:
                match = re.search(r'"translation"\s*:\s*"((?:[^"\\]|\\.)*)"', raw)
                return match.group(1).replace('\\"', '"').replace('\\', '') if match else ""
        except Exception as e:
            logger.warning(f"âš ï¸ ç¬¬ä¸€éç¿»è¯‘å¤±è´¥: {e}")
            return ""
    
    def refine_translation(self, original, first_translation, target_duration, target_language='ç®€ä½“ä¸­æ–‡'):
        if not first_translation:
            return original
        max_chars = int(target_duration * 4.5)
        current_chars = len(first_translation)
        prompt = f"""ä½ æ˜¯ç¿»è¯‘è´¨é‡å®¡æ ¡ä¸“å®¶ã€‚è¯·ä¼˜åŒ–ä»¥ä¸‹ç¿»è¯‘ï¼Œä½¿å…¶æ›´åŠ è‡ªç„¶æµç•…ã€‚

åŸæ–‡: {original}

åˆè¯‘: {first_translation}

ä¼˜åŒ–è¦æ±‚:
1. ä¿æŒåŸæ„ä¸å˜
2. æ›´åŠ å£è¯­åŒ–ã€è‡ªç„¶
3. é•¿åº¦æ§åˆ¶åœ¨ {max_chars} ä¸ªæ±‰å­—å†…ï¼ˆå½“å‰ {current_chars} å­—ï¼‰
4. å»é™¤å†—ä½™ï¼Œä½¿ç”¨æ›´ç®€æ´çš„è¡¨è¾¾

è¾“å‡ºJSON: {{"refined": "ä¼˜åŒ–åçš„è¯‘æ–‡"}}"""
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šç¿»è¯‘å®¡æ ¡ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=200
            )
            raw = response.choices[0].message.content.strip()
            try:
                data = json.loads(raw)
                refined = data.get('refined', first_translation)
            except:
                match = re.search(r'"refined"\s*:\s*"((?:[^"\\]|\\.)*)"', raw)
                refined = match.group(1) if match else first_translation
            return refined.replace('\\"', '"').replace('\\', '')
        except Exception as e:
            logger.warning(f"âš ï¸ ç¿»è¯‘ä¼˜åŒ–å¤±è´¥: {e}")
            return first_translation
    
    def translate_with_quality_check(self, text, context_prev, context_next, terms, target_duration, target_language='ç®€ä½“ä¸­æ–‡'):
        first_pass = self.translate_first_pass(text, context_prev, context_next, terms, target_duration, target_language)
        if not first_pass:
            return text, False
        refined = self.refine_translation(text, first_pass, target_duration, target_language)
        final = self.term_manager.apply_terms(refined)
        is_good = self._quality_check(text, final, target_duration)
        return final, is_good
    
    def _quality_check(self, source, target, target_duration):
        if not target or len(target) < 3:
            return False
        max_chars = int(target_duration * 5.0)
        if len(target) > max_chars:
            return False
        english_chars = sum(1 for c in target if c.isalpha() and ord(c) < 128)
        if english_chars > len(target) * 0.3:
            return False
        return True


# ===== æ–°å¢ï¼šè‡ªåŠ¨æå–å…‹éš†å‚è€ƒéŸ³é¢‘ =====
def extract_speaker_clips(folder: str, max_duration: float = 30.0):
    """
    ä¸ºæ¯ä¸ªè¯´è¯äººæå–ä¸€æ®µ <= max_duration ç§’çš„å¹²å‡€è¯­éŸ³ï¼Œç”¨äº TTS å…‹éš†
    ä¿å­˜ä¸º SPEAKER/SPEAKER_XX_CLONE.wav
    """
    if not HAS_LIBROSA:
        logger.warning("âš ï¸ æœªå®‰è£… librosaï¼Œè·³è¿‡å…‹éš†éŸ³é¢‘æå–")
        return

    transcript_path = os.path.join(folder, 'translation.json')
    vocals_path = os.path.join(folder, 'audio_vocals.wav')
    speaker_dir = os.path.join(folder, 'SPEAKER')
    
    if not os.path.exists(transcript_path) or not os.path.exists(vocals_path):
        logger.warning("âš ï¸ ç¼ºå°‘ translation.json æˆ– audio_vocals.wavï¼Œè·³è¿‡å…‹éš†éŸ³é¢‘æå–")
        return
    
    if not os.path.exists(speaker_dir):
        os.makedirs(speaker_dir)

    # åŠ è½½äººå£°éŸ³é¢‘
    try:
        vocals, sr = librosa.load(vocals_path, sr=16000)
    except Exception as e:
        logger.error(f"âŒ æ— æ³•åŠ è½½äººå£°éŸ³é¢‘: {e}")
        return

    # æŒ‰è¯´è¯äººåˆ†ç»„ç‰‡æ®µï¼ˆé€‰æ‹©æœ€é•¿ä¸” <= max_duration çš„ï¼‰
    with open(transcript_path, 'r', encoding='utf-8') as f:
        transcript = json.load(f)
    
    speaker_segments = {}
    for line in transcript:
        speaker = line.get('speaker', 'SPEAKER_00')
        start = float(line.get('start', 0))
        end = float(line.get('end', 0))
        text = line.get('text', '').strip()
        
        if not text or end - start <= 0.5:  # å¿½ç•¥å¤ªçŸ­æˆ–ç©ºæ–‡æœ¬
            continue
        
        duration = end - start
        if duration > max_duration:  # è¶…é•¿åˆ™è·³è¿‡ï¼ˆæˆ–å¯è£å‰ªï¼Œä½†ç®€å•èµ·è§è·³è¿‡ï¼‰
            continue
        
        if speaker not in speaker_segments or duration > speaker_segments[speaker]['duration']:
            speaker_segments[speaker] = {
                'start': start,
                'end': end,
                'text': text,
                'duration': duration
            }
    
    # ä¿å­˜æ¯ä¸ªè¯´è¯äººçš„æœ€ä½³ç‰‡æ®µ
    for speaker, seg in speaker_segments.items():
        start_sample = int(seg['start'] * sr)
        end_sample = int(seg['end'] * sr)
        clip = vocals[start_sample:end_sample]
        
        if len(clip) == 0:
            continue
        
        output_path = os.path.join(speaker_dir, f"{speaker}_CLONE.wav")
        try:
            import soundfile as sf
            sf.write(output_path, clip, sr)
            logger.info(f"ğŸ”Š ä¿å­˜å…‹éš†éŸ³é¢‘: {output_path} ({seg['duration']:.1f}s)")
            
            # åŒæ—¶ä¿å­˜æ–‡æœ¬ï¼ˆç”¨äº VoxCPM çš„ prompt_textï¼‰
            txt_path = output_path.replace('.wav', '.txt')
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(seg['text'])
            logger.info(f"ğŸ“„ ä¿å­˜å…‹éš†æ–‡æœ¬: {txt_path}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")


# ===== ä¸»ç¿»è¯‘å‡½æ•° =====
def translate_advanced(folder: str, target_language: str = 'ç®€ä½“ä¸­æ–‡') -> bool:
    translation_path = os.path.join(folder, 'translation.json')
    if os.path.exists(translation_path):
        logger.info(f"âœ… ç¿»è¯‘å·²å­˜åœ¨: {folder}")
        return True
    
    transcript_path = os.path.join(folder, 'transcript.json')
    if not os.path.exists(transcript_path):
        logger.error(f"âŒ å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {transcript_path}")
        return False
    
    with open(transcript_path, 'r', encoding='utf-8') as f:
        transcript = json.load(f)
    logger.info(f"ğŸ“„ åŠ è½½äº† {len(transcript)} æ¡å­—å¹•")
    
    global_terms = analyze_transcript(transcript, target_language)
    translator = AdvancedTranslator()
    full_text = ' '.join(line.get('text', '') for line in transcript)
    domain_terms = translator.term_manager.extract_terms(full_text, target_language)
    all_terms = {**domain_terms, **global_terms}
    translator.term_manager.terms = all_terms
    
    terms_path = os.path.join(folder, 'terminology.json')
    with open(terms_path, 'w', encoding='utf-8') as f:
        json.dump(all_terms, f, indent=2, ensure_ascii=False)
    
    translations = []
    quality_flags = []
    
    for i, line in enumerate(transcript):
        text = line.get('text', '').strip()
        if not text:
            translations.append("")
            quality_flags.append(False)
            continue
        
        context_prev = [transcript[j].get('text', '') for j in range(max(0, i-2), i)]
        context_next = [transcript[j].get('text', '') for j in range(i+1, min(len(transcript), i+3))]
        
        start = float(line.get('start', 0))
        end = float(line.get('end', 0))
        vad_duration = line.get('vad_duration')
        target_duration = min(float(vad_duration), end - start) if vad_duration else (end - start)
        
        progress = (i + 1) / len(transcript) * 100
        logger.info(f"ğŸ“ˆ [{i+1}/{len(transcript)}] ({progress:.1f}%) - {text[:50]}...")
        
        translation, is_good = translator.translate_with_quality_check(
            text, context_prev, context_next, all_terms,
            target_duration, target_language
        )
        
        translations.append(translation)
        quality_flags.append(is_good)
        
        logger.info(f"ğŸ’¬ è¯‘æ–‡: {translation}")
        logger.info(f"     è´¨é‡: {'âœ…' if is_good else 'âš ï¸ '} | æ—¶é•¿: {target_duration:.1f}s")
        logger.info("-" * 60)
        time.sleep(0.2)
    
    result = []
    for i, line in enumerate(transcript):
        result.append({
            "start": float(line.get('start', 0)),
            "end": float(line.get('end', 0)),
            "text": line.get('text', ''),
            "speaker": line.get('speaker', ''),
            "translation": translations[i]
        })
    
    with open(translation_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    total = len(quality_flags)
    success = sum(quality_flags)
    stats = {'total': total, 'success': success, 'success_rate': round(100 * success / total, 2) if total else 0}
    stats_path = os.path.join(folder, 'translation_stats.json')
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    logger.success(f"âœ… ç¿»è¯‘å®Œæˆ: {translation_path}")
    logger.info(f"ğŸ“Š æˆåŠŸç‡: {stats['success_rate']:.1f}% ({success}/{total})")
    
    # === æ–°å¢ï¼šè‡ªåŠ¨æå–å…‹éš†éŸ³é¢‘ ===
    logger.info("âœ‚ï¸ æ­£åœ¨æå–è¯´è¯äººå…‹éš†éŸ³é¢‘...")
    extract_speaker_clips(folder, max_duration=10.0)
    
    return True


def translate_all_advanced(root_folder: str, target_language: str = 'ç®€ä½“ä¸­æ–‡') -> int:
    folders = [
        root for root, _, files in os.walk(root_folder)
        if 'transcript.json' in files and 'translation.json' not in files
    ]
    logger.info(f"ğŸ¯ æ‰¾åˆ° {len(folders)} ä¸ªå¾…ç¿»è¯‘è§†é¢‘")
    success_count = 0
    for i, folder in enumerate(folders, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¬ å¤„ç† ({i}/{len(folders)}): {folder}")
        if translate_advanced(folder, target_language):
            success_count += 1
        if i < len(folders):
            time.sleep(2)
    logger.success(f"ğŸ å®Œæˆ! æˆåŠŸç¿»è¯‘ {success_count}/{len(folders)} ä¸ªè§†é¢‘")
    return success_count


if __name__ == '__main__':
    logger.remove()
    logger.add(
        sys.stderr,
        level="INFO",
        format="<green>{time:MM-DD HH:mm:ss}</green> | <level>{level: <6}</level> | <cyan>{message}</cyan>"
    )
    translate_all_advanced('videos', 'ç®€ä½“ä¸­æ–‡')