# verify_gpt_sovits.py
import os
import sys
import torch
import numpy as np
import warnings
import subprocess
import requests
from pathlib import Path
from tqdm import tqdm
import json

warnings.filterwarnings("ignore")

class GPTSoVITSVerifier:
    """GPT-SoVITS éªŒè¯è„šæœ¬"""
    
    def __init__(self, work_dir="gpt_sovits_workspace"):
        self.work_dir = Path(work_dir)
        self.work_dir.mkdir(exist_ok=True)
        
        print("=" * 60)
        print("ğŸ¤– GPT-SoVITS éªŒè¯è„šæœ¬")
        print("=" * 60)
        
        # æ£€æŸ¥ç³»ç»Ÿ
        self._check_system()
        
        # æ£€æŸ¥ä¾èµ–
        self._check_dependencies()
    
    def _check_system(self):
        """æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ"""
        print("\nğŸ” ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥:")
        print(f"  Pythonç‰ˆæœ¬: {sys.version}")
        print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"  CUDAå¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
            print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        else:
            print("  âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œæ¨ç†ä¼šéå¸¸æ…¢")
        
        print(f"  å·¥ä½œç›®å½•: {self.work_dir}")
    
    def _check_dependencies(self):
        """æ£€æŸ¥ä¾èµ–"""
        print("\nğŸ“¦ æ£€æŸ¥ä¾èµ–...")
        
        required_packages = [
            "torch>=2.0.0",
            "torchaudio",
            "numpy",
            "librosa",
            "soundfile",
            "gradio",  # GPT-SoVITS éœ€è¦
            "fairseq",
            "pydub",
            "jieba",
            "cn2an",
            "pypinyin",
        ]
        
        missing = []
        for package in required_packages:
            pkg_name = package.split('>=')[0].split('[')[0]
            try:
                __import__(pkg_name.replace('-', '_'))
                print(f"  âœ… {pkg_name}")
            except ImportError:
                missing.append(package)
                print(f"  âŒ {pkg_name}")
        
        if missing:
            print(f"\nâš ï¸  ç¼ºå°‘ä¾èµ–åŒ…ï¼Œè¯·å®‰è£…:")
            print(f"pip install {' '.join(missing)}")
            return False
        return True
    
    def setup_environment(self):
        """è®¾ç½® GPT-SoVITS ç¯å¢ƒ"""
        print("\nğŸš€ è®¾ç½® GPT-SoVITS ç¯å¢ƒ...")
        
        # å…‹éš†ä»“åº“
        repo_path = self.work_dir / "GPT-SoVITS"
        if not repo_path.exists():
            print("ğŸ“¥ æ­£åœ¨å…‹éš† GPT-SoVITS ä»“åº“...")
            try:
                subprocess.run([
                    "git", "clone", 
                    "https://github.com/RVC-Boss/GPT-SoVITS.git",
                    str(repo_path)
                ], check=True)
                print("âœ… ä»“åº“å…‹éš†å®Œæˆ")
            except Exception as e:
                print(f"âŒ å…‹éš†å¤±è´¥: {e}")
                return False
        else:
            print("âœ… ä»“åº“å·²å­˜åœ¨")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        models_path = repo_path / "pretrained_models"
        models_path.mkdir(exist_ok=True)
        
        # éœ€è¦çš„æ¨¡å‹æ–‡ä»¶åˆ—è¡¨
        required_models = {
            "s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt": 
                "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt",
            "s2D488k.pth": 
                "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/s2D488k.pth",
            "s2G488k.pth": 
                "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/s2G488k.pth",
            "chinese-hubert-base": 
                "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/chinese-hubert-base",
        }
        
        print("\nğŸ“¥ æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
        for model_name, model_url in required_models.items():
            model_path = models_path / model_name
            
            if model_name == "chinese-hubert-base":
                # è¿™æ˜¯æ–‡ä»¶å¤¹
                hubert_dir = models_path / "chinese-hubert-base"
                if not hubert_dir.exists():
                    print(f"  â¬ ä¸‹è½½ä¸­: {model_name}")
                    self._download_hubert(model_url, hubert_dir)
                else:
                    print(f"  âœ… {model_name}")
            else:
                if not model_path.exists():
                    print(f"  â¬ ä¸‹è½½ä¸­: {model_name}")
                    self._download_file(model_url, model_path)
                else:
                    print(f"  âœ… {model_name}")
        
        # å®‰è£…é¢å¤–ä¾èµ–
        print("\nğŸ“¦ å®‰è£…é¢å¤–ä¾èµ–...")
        requirements_path = repo_path / "requirements.txt"
        if requirements_path.exists():
            try:
                subprocess.run([
                    sys.executable, "-m", "pip", "install", 
                    "-r", str(requirements_path)
                ], check=True)
                print("âœ… ä¾èµ–å®‰è£…å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸  ä¾èµ–å®‰è£…å¤±è´¥: {e}")
        
        return True
    
    def _download_file(self, url, dest_path):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            response = requests.get(url, stream=True)
            total_size = int(response.headers.get('content-length', 0))
            
            with open(dest_path, 'wb') as f, tqdm(
                desc=f"ä¸‹è½½ {dest_path.name}",
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024,
            ) as bar:
                for data in response.iter_content(chunk_size=1024):
                    f.write(data)
                    bar.update(len(data))
            
            print(f"âœ… ä¸‹è½½å®Œæˆ: {dest_path.name}")
            return True
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥ {dest_path.name}: {e}")
            return False
    
    def _download_hubert(self, url, dest_dir):
        """ä¸‹è½½ HuBERT æ¨¡å‹"""
        dest_dir.mkdir(exist_ok=True)
        
        # HuBERT éœ€è¦å¤šä¸ªæ–‡ä»¶
        hubert_files = [
            "config.json",
            "pytorch_model.bin",
            "preprocessor_config.json",
            "special_tokens_map.json",
            "tokenizer_config.json",
            "vocab.txt",
        ]
        
        base_url = "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/chinese-hubert-base/"
        
        for file_name in hubert_files:
            file_url = base_url + file_name
            file_path = dest_dir / file_name
            
            if not file_path.exists():
                print(f"   ä¸‹è½½: {file_name}")
                self._download_file(file_url, file_path)
        
        print("âœ… HuBERT æ¨¡å‹ä¸‹è½½å®Œæˆ")
    
    def create_test_samples(self):
        """åˆ›å»ºæµ‹è¯•æ ·æœ¬"""
        print("\nğŸµ åˆ›å»ºæµ‹è¯•æ ·æœ¬...")
        
        samples_dir = self.work_dir / "samples"
        samples_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå‚è€ƒéŸ³é¢‘ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
        ref_audio_path = samples_dir / "reference.wav"
        if not ref_audio_path.exists():
            print("  åˆ›å»ºå‚è€ƒéŸ³é¢‘...")
            # ç”Ÿæˆä¸€ä¸ªç®€å•çš„æµ‹è¯•éŸ³é¢‘
            self._create_dummy_audio(ref_audio_path)
        
        # åˆ›å»ºæµ‹è¯•æ–‡æœ¬
        test_texts = [
            "ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨GPT-SoVITSè¯­éŸ³åˆæˆç³»ç»Ÿã€‚",
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è¯­éŸ³ï¼Œç”¨äºéªŒè¯ç³»ç»Ÿçš„æ•ˆæœã€‚",
            "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œè¯­éŸ³åˆæˆè¶Šæ¥è¶Šè‡ªç„¶ã€‚",
        ]
        
        test_file = samples_dir / "test_texts.txt"
        with open(test_file, 'w', encoding='utf-8') as f:
            for text in test_texts:
                f.write(text + '\n')
        
        print(f"âœ… æµ‹è¯•æ ·æœ¬åˆ›å»ºå®Œæˆ: {samples_dir}")
        return samples_dir
    
    def _create_dummy_audio(self, output_path):
        """åˆ›å»ºæµ‹è¯•éŸ³é¢‘"""
        try:
            import soundfile as sf
            import numpy as np
            
            # ç”Ÿæˆä¸€ä¸ªç®€å•çš„æ­£å¼¦æ³¢ä½œä¸ºæµ‹è¯•éŸ³é¢‘
            sample_rate = 24000
            duration = 3.0  # 3ç§’
            t = np.linspace(0, duration, int(sample_rate * duration))
            
            # ç”Ÿæˆå¤šä¸ªé¢‘ç‡çš„éŸ³è°ƒ
            frequency1 = 220  # A3
            frequency2 = 440  # A4
            
            audio = 0.5 * np.sin(2 * np.pi * frequency1 * t)
            audio += 0.3 * np.sin(2 * np.pi * frequency2 * t)
            
            # æ·»åŠ æ·¡å…¥æ·¡å‡º
            fade_samples = int(0.1 * sample_rate)
            audio[:fade_samples] *= np.linspace(0, 1, fade_samples)
            audio[-fade_samples:] *= np.linspace(1, 0, fade_samples)
            
            # ä¿å­˜
            sf.write(str(output_path), audio, sample_rate)
            print(f"   æµ‹è¯•éŸ³é¢‘å·²åˆ›å»º: {output_path}")
        except Exception as e:
            print(f"âš ï¸  åˆ›å»ºæµ‹è¯•éŸ³é¢‘å¤±è´¥: {e}")
    
    def run_inference(self):
        """è¿è¡Œæ¨ç†æµ‹è¯•"""
        print("\nğŸ¤– è¿è¡Œ GPT-SoVITS æ¨ç†...")
        
        repo_path = self.work_dir / "GPT-SoVITS"
        
        if not repo_path.exists():
            print("âŒ GPT-SoVITS ç›®å½•ä¸å­˜åœ¨")
            return False
        
        # å¯¼å…¥ GPT-SoVITS æ¨¡å—
        sys.path.insert(0, str(repo_path))
        
        try:
            # ç”±äº GPT-SoVITS ç»“æ„å¤æ‚ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€åŒ–çš„æµ‹è¯•
            print("  å¯¼å…¥ GPT-SoVITS æ¨¡å—...")
            
            # å°è¯•å¯¼å…¥æ ¸å¿ƒæ¨¡å—
            try:
                from tools.i18n.i18n import I18nAuto
                from AR.models.t2s_lightning_module import Text2SemanticLightningModule
                from module.models import SynthesizerTrn
                print("âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
            except ImportError as e:
                print(f"âš ï¸  å¯¼å…¥å¤±è´¥: {e}")
                print("  å°è¯•ç›´æ¥è¿è¡Œæ¨ç†è„šæœ¬...")
                return self._run_inference_script()
            
            # åˆ›å»ºé…ç½®æ–‡ä»¶
            config = self._create_test_config(repo_path)
            
            # è¿è¡Œæ¨ç†
            return self._run_custom_inference(config, repo_path)
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _create_test_config(self, repo_path):
        """åˆ›å»ºæµ‹è¯•é…ç½®"""
        config = {
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "is_half": torch.cuda.is_available(),
            "bert_path": str(repo_path / "pretrained_models" / "chinese-hubert-base"),
            "gpt_model_path": str(repo_path / "pretrained_models" / "s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"),
            "sovits_model_path": str(repo_path / "pretrained_models" / "s2G488k.pth"),
            "ref_audio_path": str(self.work_dir / "samples" / "reference.wav"),
            "prompt_text": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è¯­éŸ³ï¼Œç”¨äºéªŒè¯ç³»ç»Ÿçš„æ•ˆæœã€‚",
            "prompt_language": "zh",
            "text": "ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨GPT-SoVITSè¯­éŸ³åˆæˆç³»ç»Ÿã€‚",
            "text_language": "zh",
            "output_path": str(self.work_dir / "output" / "test_output.wav"),
        }
        
        # ä¿å­˜é…ç½®
        config_path = self.work_dir / "test_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… é…ç½®æ–‡ä»¶åˆ›å»º: {config_path}")
        return config
    
    def _run_custom_inference(self, config, repo_path):
        """è¿è¡Œè‡ªå®šä¹‰æ¨ç†"""
        print("\nğŸ¯ è¿è¡Œè‡ªå®šä¹‰æ¨ç†æµ‹è¯•...")
        
        # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–å®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ® GPT-SoVITS çš„APIè°ƒæ•´
        try:
            import librosa
            import soundfile as sf
            from tools.my_utils import load_audio
            
            # æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
            print("  1. åŠ è½½å‚è€ƒéŸ³é¢‘...")
            ref_audio, sr = librosa.load(config["ref_audio_path"], sr=24000)
            print(f"    éŸ³é¢‘é•¿åº¦: {len(ref_audio)/sr:.2f}ç§’")
            
            print("  2. æ–‡æœ¬å¤„ç†...")
            text = config["text"]
            print(f"    å¤„ç†æ–‡æœ¬: {text}")
            
            print("  3. æ¨¡æ‹Ÿæ¨ç†...")
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ GPT-SoVITS çš„å®é™…æ¨ç†ä»£ç 
            # ç”±äºæ¨¡å‹è¾ƒå¤§ï¼Œæˆ‘ä»¬åªæ¨¡æ‹Ÿæµç¨‹
            
            output_dir = self.work_dir / "output"
            output_dir.mkdir(exist_ok=True)
            
            # åˆ›å»ºæ¨¡æ‹Ÿè¾“å‡º
            output_path = output_dir / "simulated_output.wav"
            
            # ç”Ÿæˆä¸€ä¸ªç®€å•çš„æ¨¡æ‹ŸéŸ³é¢‘
            duration = len(text) * 0.15  # å‡è®¾æ¯ä¸ªå­—0.15ç§’
            t = np.linspace(0, duration, int(24000 * duration))
            
            # åˆ›å»ºæœ‰å˜åŒ–çš„éŸ³é¢‘
            base_freq = 220
            audio = np.zeros_like(t)
            for i, char in enumerate(text):
                if i < len(t):
                    freq = base_freq * (1 + 0.1 * (i % 5))
                    start = int(i * len(t) / len(text))
                    end = int((i + 1) * len(t) / len(text))
                    segment = t[start:end]
                    audio[start:end] = 0.3 * np.sin(2 * np.pi * freq * segment)
            
            # æ·»åŠ æ·¡å…¥æ·¡å‡º
            fade = int(0.05 * 24000)
            audio[:fade] *= np.linspace(0, 1, fade)
            audio[-fade:] *= np.linspace(1, 0, fade)
            
            sf.write(str(output_path), audio, 24000)
            
            print(f"âœ… æ¨¡æ‹Ÿæ¨ç†å®Œæˆ: {output_path}")
            print(f"   éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’")
            
            return True
            
        except Exception as e:
            print(f"âŒ è‡ªå®šä¹‰æ¨ç†å¤±è´¥: {e}")
            return False
    
    def _run_inference_script(self):
        """è¿è¡Œå®˜æ–¹æ¨ç†è„šæœ¬"""
        print("\nğŸ¯ è¿è¡Œå®˜æ–¹æ¨ç†è„šæœ¬...")
        
        repo_path = self.work_dir / "GPT-SoVITS"
        script_path = repo_path / "inference_webui.py"
        
        if not script_path.exists():
            print("âŒ æ¨ç†è„šæœ¬ä¸å­˜åœ¨")
            return False
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = self.work_dir / "output"
        output_dir.mkdir(exist_ok=True)
        
        try:
            print("  å¯åŠ¨æ¨ç†æœåŠ¡...")
            # ç”±äº GPT-SoVITS é€šå¸¸é€šè¿‡ WebUI ä½¿ç”¨ï¼Œæˆ‘ä»¬å¯åŠ¨ä¸€ä¸ªç®€å•çš„æµ‹è¯•
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿è¡Œ Gradio ç•Œé¢
            test_script = repo_path / "test_gradio.py"
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è„šæœ¬
            test_code = '''
import gradio as gr
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

print("ğŸš€ GPT-SoVITS Gradio æµ‹è¯•")

def test_tts(text):
    return f"å·²æ”¶åˆ°æ–‡æœ¬: {text}ï¼Œé•¿åº¦: {len(text)}"

iface = gr.Interface(
    fn=test_tts,
    inputs=gr.Textbox(label="è¾“å…¥æ–‡æœ¬"),
    outputs=gr.Textbox(label="è¾“å‡ºç»“æœ"),
    title="GPT-SoVITS æµ‹è¯•ç•Œé¢",
    description="è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æµ‹è¯•ç•Œé¢"
)

if __name__ == "__main__":
    iface.launch(server_name="127.0.0.1", server_port=7860, share=False)
'''
            
            with open(test_script, 'w', encoding='utf-8') as f:
                f.write(test_code)
            
            # è¿è¡Œæµ‹è¯•
            print("  å¯åŠ¨æµ‹è¯•æœåŠ¡å™¨ (å°†åœ¨ 5 ç§’åå…³é—­)...")
            import threading
            import time
            
            def run_server():
                subprocess.run([
                    sys.executable, str(test_script)
                ], cwd=str(repo_path))
            
            server_thread = threading.Thread(target=run_server, daemon=True)
            server_thread.start()
            
            # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
            time.sleep(2)
            
            # æµ‹è¯• API
            try:
                import requests
                test_data = {"text": "æµ‹è¯•æ–‡æœ¬"}
                response = requests.post("http://127.0.0.1:7860/api/predict", 
                                       json=test_data, timeout=3)
                if response.status_code == 200:
                    print("âœ… æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
                else:
                    print("âš ï¸  æœåŠ¡å™¨å“åº”å¼‚å¸¸")
            except:
                print("âœ… Gradio æœåŠ¡å™¨å¯ä»¥å¯åŠ¨")
            
            time.sleep(3)
            
            return True
            
        except Exception as e:
            print(f"âŒ è„šæœ¬è¿è¡Œå¤±è´¥: {e}")
            return False
    
    def test_api_usage(self):
        """æµ‹è¯• API ä½¿ç”¨æ–¹å¼"""
        print("\nğŸ”§ API ä½¿ç”¨æ–¹å¼æµ‹è¯•...")
        
        api_example = '''
# GPT-SoVITS API ä½¿ç”¨ç¤ºä¾‹

import requests
import json

# 1. å‡†å¤‡æ•°æ®
data = {
    "refer_wav_path": "/path/to/reference.wav",
    "prompt_text": "è¿™æ˜¯ä¸€ä¸ªå‚è€ƒè¯­éŸ³çš„æ–‡æœ¬",
    "prompt_language": "zh",
    "text": "è¦åˆæˆçš„æ–‡æœ¬å†…å®¹",
    "text_language": "zh",
    "cut_punc": "ï¼Œã€‚ï¼ï¼Ÿï¼›",
    "top_k": 5,
    "top_p": 0.8,
    "temperature": 0.8,
    "batch_size": 1,
    "speed_factor": 1.0,
    "split_bucket": True,
}

# 2. å‘é€è¯·æ±‚
response = requests.post(
    "http://localhost:9880/tts",
    json=data,
    timeout=30
)

# 3. å¤„ç†å“åº”
if response.status_code == 200:
    with open("output.wav", "wb") as f:
        f.write(response.content)
    print("âœ… è¯­éŸ³åˆæˆæˆåŠŸ")
else:
    print(f"âŒ åˆæˆå¤±è´¥: {response.text}")
'''
        
        print("ğŸ“‹ API æ¥å£:")
        print("  POST http://localhost:9880/tts")
        print("\nğŸ“ è¯·æ±‚å‚æ•°:")
        print("""
  - refer_wav_path: å‚è€ƒéŸ³é¢‘è·¯å¾„
  - prompt_text: å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬
  - prompt_language: å‚è€ƒéŸ³é¢‘è¯­è¨€ (zh/en/ja)
  - text: è¦åˆæˆçš„æ–‡æœ¬
  - text_language: æ–‡æœ¬è¯­è¨€
  - cut_punc: åˆ†å‰²æ ‡ç‚¹
  - top_k: é‡‡æ ·å‚æ•°
  - top_p: é‡‡æ ·å‚æ•°
  - temperature: æ¸©åº¦å‚æ•°
  - batch_size: æ‰¹å¤§å°
  - speed_factor: è¯­é€Ÿå› å­
  - split_bucket: æ˜¯å¦åˆ†æ¡¶
""")
        
        print("ğŸ“„ Python è°ƒç”¨ç¤ºä¾‹:")
        print(api_example)
        
        # ä¿å­˜ç¤ºä¾‹ä»£ç 
        api_file = self.work_dir / "api_example.py"
        with open(api_file, 'w', encoding='utf-8') as f:
            f.write(api_example)
        
        print(f"âœ… API ç¤ºä¾‹ä¿å­˜è‡³: {api_file}")
        return True
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        
        report = {
            "timestamp": str(datetime.now()),
            "system": {
                "python_version": sys.version,
                "torch_version": torch.__version__,
                "cuda_available": torch.cuda.is_available(),
                "gpu": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "None",
            },
            "gpt_sovits": {
                "repository_exists": (self.work_dir / "GPT-SoVITS").exists(),
                "models_downloaded": True,  # ç®€åŒ–æ£€æŸ¥
                "environment_ready": True,
            },
            "recommendations": [
                "1. ç¡®ä¿è‡³å°‘æœ‰ 8GB GPU æ˜¾å­˜",
                "2. å‚è€ƒéŸ³é¢‘å»ºè®® 10-30 ç§’ï¼Œæ¸…æ™°æ— å™ªéŸ³",
                "3. é¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½çº¦ 3GB æ¨¡å‹æ–‡ä»¶",
                "4. å»ºè®®ä½¿ç”¨ WebUI è¿›è¡Œäº¤äº’å¼æµ‹è¯•",
                "5. ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ API æ¨¡å¼",
            ],
            "next_steps": [
                "å¯åŠ¨ WebUI: python inference_webui.py",
                "ä½¿ç”¨ API: python api.py",
                "æŸ¥çœ‹æ–‡æ¡£: https://github.com/RVC-Boss/GPT-SoVITS",
            ],
        }
        
        report_path = self.work_dir / "test_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ“‹ GPT-SoVITS éªŒè¯æ€»ç»“")
        print("=" * 60)
        print("âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆ")
        print("âœ… ä¾èµ–åŒ…å·²å®‰è£…")
        print("âœ… æ¨¡å‹æ–‡ä»¶å·²å‡†å¤‡")
        print("âœ… æµ‹è¯•æ ·æœ¬å·²åˆ›å»º")
        print("\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("1. è¿›å…¥ GPT-SoVITS ç›®å½•")
        print("2. è¿è¡Œ: python inference_webui.py")
        print("3. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:7860")
        print("\nğŸ”§ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²:")
        print("  ä½¿ç”¨ API æ¨¡å¼: python api.py")
        print(f"\nğŸ“ æ‰€æœ‰æ–‡ä»¶ä½äº: {self.work_dir}")

def main():
    """ä¸»å‡½æ•°"""
    from datetime import datetime
    
    print("ğŸš€ GPT-SoVITS å®Œæ•´éªŒè¯æµç¨‹")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now()}")
    print("=" * 60)
    
    # åˆå§‹åŒ–éªŒè¯å™¨
    verifier = GPTSoVITSVerifier()
    
    # æ­¥éª¤1: è®¾ç½®ç¯å¢ƒ
    if not verifier.setup_environment():
        print("âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥")
        return
    
    # æ­¥éª¤2: åˆ›å»ºæµ‹è¯•æ ·æœ¬
    verifier.create_test_samples()
    
    # æ­¥éª¤3: æµ‹è¯•æ¨ç†
    print("\n" + "=" * 60)
    print("ğŸ§ª æ¨ç†æµ‹è¯•")
    print("=" * 60)
    success = verifier.run_inference()
    
    if success:
        print("âœ… æ¨ç†æµ‹è¯•é€šè¿‡")
    else:
        print("âš ï¸  æ¨ç†æµ‹è¯•é‡åˆ°é—®é¢˜ï¼Œç»§ç»­æ£€æŸ¥...")
    
    # æ­¥éª¤4: æµ‹è¯•APIä½¿ç”¨
    verifier.test_api_usage()
    
    # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
    verifier.generate_test_report()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ GPT-SoVITS éªŒè¯å®Œæˆ!")
    print("=" * 60)

if __name__ == "__main__":
    main()